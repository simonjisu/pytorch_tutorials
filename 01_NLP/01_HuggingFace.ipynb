{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Tutorial\n",
    "\n",
    "This is a tutorial for me to learn how to use transformer with huggingface.\n",
    "\n",
    "# Reference: \n",
    "- https://huggingface.co/\n",
    "- https://huggingface.co/transformers/\n",
    "- https://github.com/huggingface/datasets\n",
    "- https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB?usp=sharing#scrollTo=ImupuGXDGq7b\n",
    "- https://github.com/monologg/KoELECTRA\n",
    "- https://github.com/monologg/KoELECTRA/blob/master/finetune/run_squad.py\n",
    "- Korean Sentence Splitter: https://github.com/hyunwoongko/kss\n",
    "\n",
    "# Installation\n",
    "\n",
    "```bash\n",
    "$ pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0\n",
      "TorchText Version: 0.8.0a0+c851c3e\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"TorchText Version: {torchtext.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "need to install sentencepiece\n",
    "\n",
    "```bash\n",
    "$ pip install sentencepiece\n",
    "$ pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use?\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "- ConversationalPipeline\n",
    "- FeatureExtractionPipeline\n",
    "- FillMaskPipeline\n",
    "- QuestionAnsweringPipeline\n",
    "- SummarizationPipeline\n",
    "- TextClassificationPipeline\n",
    "- TextGenerationPipeline\n",
    "- TokenClassificationPipeline\n",
    "- TranslationPipeline\n",
    "- ZeroShotClassificationPipeline\n",
    "- Text2TextGenerationPipeline\n",
    "- TableQuestionAnsweringPipeline\n",
    "\n",
    "function: `pipeline`\n",
    "\n",
    "- \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "- \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "- \"ner\": will return a TokenClassificationPipeline.\n",
    "- \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "- \"fill-mask\": will return a FillMaskPipeline.\n",
    "- \"summarization\": will return a SummarizationPipeline.\n",
    "- \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "- \"text2text-generation\": will return a Text2TextGenerationPipeline.\n",
    "- \"text-generation\": will return a TextGenerationPipeline.\n",
    "- \"zero-shot-classification:: will return a ZeroShotClassificationPipeline.\n",
    "- \"conversation\": will return a ConversationalPipeline.\n",
    "\n",
    "model will be automatically downloaded in `~/.cache/huggingface/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"We are very happy to show you the ü§ó Transformers library.\",\n",
    "    \"I'll go to Apple Store.\",\n",
    "    \"This model covers a lot area. But, I won't use it. Since it is too hard to use.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier(sentences)\n",
    "for res in results:\n",
    "    print(f\"label: {res['label']}, with score: {round(res['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Custom Dataset\n",
    "\n",
    "https://huggingface.co/transformers/custom_datasets.html#qa-squad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "- Ï†úÎ™©(title)\n",
    "- Î≥∏Î¨∏Ïùò Ïπ¥ÌÖåÍ≥†Î¶¨(source)\n",
    "- Î≥∏Î¨∏(context)\n",
    "- ÏßàÎ¨∏ Î≤àÌò∏(id)\n",
    "- Ïú°ÌïòÏõêÏπô(classtype)\n",
    "- ÏßàÎ¨∏(question)\n",
    "- Ï†ïÎãµÏùò ÏãúÏûëÏúÑÏπò(answer_start)\n",
    "- Ï†ïÎãµ(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_normal_squad_all.json\n",
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_clue0529_squad_all.json\n",
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_noanswer_squad_all.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "repo_path = Path().absolute().parent\n",
    "data_path = repo_path.parent / \"data\" / \"AIhub\" / \"QA\"\n",
    "for p in data_path.glob(\"ko*.json\"):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in tqdm(squad_dict[\"data\"], total=len(squad_dict[\"data\"]), desc=\"Reading Dataset\"):\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47314/47314 [00:00<00:00, 444901.71it/s]\n",
      "Reading Dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:00<00:00, 548757.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_file = \"ko_nia_normal_squad_all.json\"\n",
    "train_path = data_path / train_file\n",
    "val_file = \"ko_nia_clue0529_squad_all.json\"\n",
    "val_path = data_path / val_file\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad(train_path)\n",
    "val_contexts, val_questions, val_answers = read_squad(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243425 96663\n"
     ]
    }
   ],
   "source": [
    "print(len(train_contexts), len(val_contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mContext: \u001b[0m\n",
      "ÎåìÍ∏Ä Ï°∞Ïûë ÏùòÌòπ ÏÇ¨Í±¥ÏúºÎ°ú Íµ¨ÏÜçÎêú ÍπÄÎ™®(48¬∑ÎãâÎÑ§ÏûÑ ÎìúÎ£®ÌÇπ)Ïî®Îäî Í∑∏ÎèôÏïà ÌéòÏù¥Ïä§Î∂ÅÏùÑ ÌÜµÌï¥ Ï∂îÎØ∏Ïï† ÎçîÎ∂àÏñ¥ÎØºÏ£ºÎãπ ÎåÄÌëú, Ìè¨ÌÑ∏ÏÇ¨Ïù¥Ìä∏ ÎÑ§Ïù¥Î≤Ñ, Î¨∏Ïû¨Ïù∏ ÎåÄÌÜµÎ†π ÌïµÏã¨ ÏßÄÏßÄÏûêÎì§Ïù∏ ‚ÄòÎ¨∏ÍøÄÏò§ÏÜåÎ¶¨‚Äô Îì±ÏùÑ Ïã∏Ïû°ÏïÑ ÎπÑÌåêÌñàÎã§. Ïù¥Î∞ñÏóêÎèÑ Ïó¨Î°† Ï†ÑÎ¨∏Í∞Ä, ÎÇ®Î∂Å Í¥ÄÍ≥Ñ Ï†ÑÎ¨∏Í∞ÄÏ≤òÎüº ÌñâÏÑ∏ÌïòÎ©¥ÏÑú Ï†ïÏπòÍ∂åÏóê ÌõàÏàòÎ•º ÎëêÎäî ÏùºÎèÑ ÎßàÎã§ÌïòÏßÄ ÏïäÏïòÎã§. ÍπÄÏî®Îäî ÏûêÏã†Ïùò ÌéòÏù¥Ïä§Î∂Å Í≥ÑÏ†ï ‚ÄòSj Kim(ÎìúÎ£®ÌÇπ)‚ÄôÏùÑ ÌÜµÌï¥ \u001b[1m\u001b[31mÏßÄÎÇú 1Ïõî 26Ïùº\u001b[0m ‚ÄúÍ∑∏ÎèôÏïà Í∑∏Î†áÍ≤å ÌïòÎùºÍ≥† Ìï¥ÎèÑ ÏïàÌïòÎçîÎãà ÎÑ§Ïù¥Î≤ÑÏóêÏÑú ÎìúÎîîÏñ¥ Í≥ÑÏ†ï Ï†ëÏÜçÍ¥ÄÎ¶¨ÌïòÍ≥† Í∏∞ÏÇ¨ ÏõπÌéòÏù¥ÏßÄÎ•º ÏÜêÎ¥§Îã§‚ÄùÎ©∞ ‚ÄúÍ∏∞Ï°¥ Îß§ÌÅ¨Î°ú Í∞ôÏùÄ Í≤ÉÏùÄ Ïù¥ÌãÄ Ï†ÑÎ∂ÄÌÑ∞ ÎßâÌòÄÏÑú Ïïà Îê† Í≤É‚ÄùÏù¥ÎùºÍ≥† Ïñ∏Í∏âÌñàÎã§. Îß§ÌÅ¨Î°úÎ•º Ïù¥Ïö©Ìïú ÎåìÍ∏Ä Ï∂îÏ≤ú Ï°∞Ïûë Î∞©Î≤ïÏùÑ ÍπÄÏî®Í∞Ä Î™ÖÎ∞±ÌïòÍ≤å ÏïåÍ≥† ÏûàÏóàÎã§Îäî Ï†êÏùÑ Îí∑Î∞õÏπ®ÌïòÎäî Í∏ÄÏù¥ÎùºÎäî Î∂ÑÏÑùÏù¥ ÎßéÎã§. ÍπÄÏî®Îäî Ïù¥ Í∏ÄÏóêÏÑú Ï∂î ÎåÄÌëúÎèÑ ÎπÑÌåêÌñàÎã§. Í∑∏Îäî ‚ÄúÏ≤≠ÏôÄÎåÄÍ∞Ä ÏïïÎ†•ÏùÑ ÎÑ£Ïñ¥ ÎÑ§Ïù¥Î≤Ñ ÏõπÌéòÏù¥ÏßÄÎ•º Í∞úÌé∏ÌïòÍ≤å ÌïòÎ©¥ Î≠ò ÌïòÎäêÎÉê‚ÄùÎ©∞ ‚Äú‚ÄòÎ¨∏Ïû¨Ïïô‚Äô Îã®Ïñ¥Î•º ÌîÑÎ†àÏûÑÌôîÌïú Í≤ÉÏùÄ Í∑∏Í±∏ Í∏∞ÏÇ¨ÌôîÏãúÌÇ® Ï∂î ÎåÄÌëúÏùò ÏûëÌíà‚ÄùÏù¥ÎùºÍ≥† ÏßÄÏ†ÅÌñàÎã§. Ïù¥Ïñ¥ ‚ÄúÏßÄÏßÄÏûêÎì§ÏùÄ Ïó¥Ïã¨Ìûà ÎåìÍ∏Ä Î∞©Ïñ¥ÌïòÍ≥† ÏûàÎäîÎç∞ Ï∂î ÎåÄÌëúÎäî Ìú¥Í∞Ä Í∞ÄÏÖ®Îã§Ï£†? ÎØºÏ£ºÎãπÏùò ÏïûÎÇ†Ïù¥ ÏïîÏö∏ÌïòÎã§‚ÄùÍ≥† ÏçºÎã§. ‚ÄòÎ¨∏ÍøÄÏò§ÏÜåÎ¶¨‚ÄôÏóê ÎåÄÌïú ÌòπÌèâÎèÑ Ïù¥Ïñ¥Ï°åÎã§. ÍπÄÏî®Îäî ÏßÄÎÇúÌï¥ 12Ïõî ÌéòÏù¥Ïä§Î∂Å Í∏ÄÏóêÏÑú ‚ÄúÏûêÏú†ÌïúÍµ≠Îãπ ÎåìÍ∏ÄÎ∂ÄÎåÄÎäî Î¨∏ ÎåÄÌÜµÎ†π Í¥ÄÎ†® Í∏∞ÏÇ¨Ïóê ÏïÖÌîåÏùÑ Îã® Îí§ ÏàúÏãùÍ∞ÑÏóê 7000‚àº8000Í∞úÏùò Ï∂îÏ≤úÏùÑ Ï∞çÎäî ÌôîÎ†•‚ÄùÏù¥ÎùºÎ©∞ ‚ÄúÎ¨∏ÍøÄÏò§ÏÜåÎ¶¨ÎÇò Îã¨ÎπõÍ∏∞ÏÇ¨Îã®(Î¨∏ ÎåÄÌÜµÎ†π ÌïµÏã¨ ÏßÄÏßÄÏ∏µ)ÏùÄ Í∏∞ÍªèÌï¥Ïïº Í∑∏ Î∞òÏùò Î∞òÏóêÎèÑ ÎØ∏ÏπòÏßÄ Î™ªÌïúÎã§‚ÄùÍ≥† ÌñàÎã§. Í∑∏Îü¨Î©¥ÏÑú ‚ÄúÏßÄÍ∏àÍπåÏßÄ Î¨∏Ïû¨Ïù∏ ÏßÄÏßÄÏûêÎì§ÏùÄ Ïò®ÎùºÏù∏ÏùÑ ÏôÑÏ†ÑÌûà Ïû•ÏïÖÌïòÍ≥† ÏûàÎã§Í≥† Ïò§ÎßåÏóê Îπ†Ï†∏ ÏûàÏóàÎã§‚ÄùÍ≥† ÏçºÎã§. Ïó¨Î°†Í≥º ÎÇ®Î∂Å Í¥ÄÍ≥ÑÏóê ÎåÄÌïú ‚ÄòÏ†êÏûñÏùÄ‚Äô ÌõàÏàòÎèÑ ÎπºÎÜìÏßÄ ÏïäÏïòÎã§. ÍπÄÏî®Îäî 1Ïõî Ï¥à ÌéòÏù¥Ïä§Î∂ÅÏóêÏÑú ‚ÄúÏò®ÎùºÏù∏ Ïó¨Î°† Ï†êÏú†Ïú®Ïù¥ ÎåÄÌÜµÎ†π ÏßÄÏßÄÏú®Ïù¥ÎùºÍ≥† Ïó¨Îü¨ Ï∞®Î°Ä Ïù¥ÏïºÍ∏∞Î•º Ìï¥ÎèÑ Ï†ïÏπòÏù∏ÏùÄ ÏïåÏïÑÎì£ÏßÄ Î™ªÌïúÎã§‚ÄùÎ©∞ ‚ÄúÏïÑÏßÅÎèÑ Ïò§ÌîÑÎùºÏù∏ ÏÑ∏ÏÉÅÏù¥ Ïó¨Î°†ÏùÑ Ï¢åÏö∞ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌïòÍ≥† ÏûàÎã§‚ÄùÍ≥† ÌñàÎã§. Ïù¥Ïñ¥ ‚ÄúÌÜµÏùºÏùÄ Î∞òÎìúÏãú Ïù¥Î§ÑÏïºÎßå Ìï† ÏàôÏõê‚ÄùÏù¥ÎùºÎ©∞ ‚ÄúÍ∑∏Îü¥ ÎïåÎäî Î∂ÅÌïúÏóê ÎåÄÌïú Î∞úÏñ∏ÎèÑ ÏòàÏùòÏûàÍ≤å Ìï¥Ïïº ÌïúÎã§. ÏöîÏ¶ò 20, 30ÎåÄÎäî Ï†ÅÎåÄÍ∞ê ÏùºÏÉâÏù¥Îùº Í±±Ï†ï‚ÄùÏù¥ÎùºÍ≥† ÌñàÎã§. ÌïòÏßÄÎßå ÍπÄÏî®Îäî Ïù¥ Í∏ÄÏùÑ Ïò¨Î¶∞ ÏßÅÌõÑÏù∏ ÏßÄÎÇú 1Ïõî 17Ïùº ÌèâÏ∞Ω ÎèôÍ≥ÑÏò¨Î¶ºÌîΩ Ïó¨Ïûê ÏïÑÏù¥Ïä§ÌïòÌÇ§ ÎÇ®Î∂Å Îã®ÏùºÌåÄ Í∏∞ÏÇ¨Ïùò ÎπÑÌåê ÎåìÍ∏ÄÏóê ‚ÄòÍ≥µÍ∞ê‚Äô ÌÅ¥Î¶≠ ÏàòÎ•º ÎäòÎ†§ Íµ¨ÏÜçÎêêÎã§. ÍπÄÏî®Í∞Ä ÏßëÏ§ëÏ†ÅÏúºÎ°ú ‚ÄòÍ≥µÍ∞ê‚ÄôÏùÑ ÎàÑÎ•∏ ÎåìÍ∏ÄÏùÄ ‚ÄòÎïÄ ÌùòÎ¶∞ ÏÑ†ÏàòÎì§Ïù¥ Î¨¥Ïä® Ï£ÑÎÉê‚Äô ‚ÄòÏ≤≠ÏôÄÎåÄ Ïó¨Îãπ Îã§ Ïã§ÏàòÌïòÎäî Í±∞Îã§. Íµ≠ÎØºÎì§ ÎøîÎÇ¨Îã§‚Äô Îì± ÎåÄÎ∂ÄÎ∂Ñ Îã®ÏùºÌåÄ Íµ¨ÏÑ±ÏùÑ ÎπÑÌåêÌïòÎäî ÎÇ¥Ïö©Ïù¥ÏóàÎã§. ÍπÄÏî®Îäî Ï∏°Í∑ºÎì§ÏóêÍ≤åÎäî Ìô©ÎãπÌïú Ïñ∏Í∏âÎèÑ Ìïú Í≤ÉÏúºÎ°ú ÏïåÎ†§Ï°åÎã§. ÍπÄÏî®Í∞Ä Ïö¥ÏòÅÌïú ‚ÄòÍ≤ΩÏ†úÏ†Å Í≥µÏßÑÌôî Î™®ÏûÑ‚Äô ÌöåÏõê Ï§ë Ìïú Î™ÖÏùÄ ÍµêÌÜµÎ∞©ÏÜ°ÎùºÎîîÏò§Ïóê Ï∂úÏó∞Ìï¥ ‚ÄúÍπÄÏî®Í∞Ä ‚ÄòÎ¨∏ ÎåÄÌÜµÎ†πÏù¥ÎÇò Ï≤≠ÏôÄÎåÄ ÌïµÏã¨ Î©§Î≤ÑÎì§Ïù¥ Ï†úÏàòÏù¥Ìä∏(ÏùåÎ™®Î°†Ïóê ÎÇòÏò§Îäî Í∞ÄÌÜ®Î¶≠ ÏÇ¨Ï†úÏßëÎã®)Ïù¥Îã§‚Äô ‚ÄòÎÖ∏Î¨¥ÌòÑ Ï†Ñ ÎåÄÌÜµÎ†π Ï£ΩÏùåÏóê Î¨∏ ÎåÄÌÜµÎ†πÏù¥ Í¥ÄÏó¨ÌñàÎã§‚ÄôÍ≥† ÎßêÌñàÎã§‚ÄùÍ≥† Ï†ÑÌñàÎã§. Îòê ‚ÄúÏïàÌù¨Ï†ï Ï†Ñ Ï∂©ÎÇ®ÏßÄÏÇ¨Í∞Ä ÎÇôÎßàÌïòÏûê ‚ÄòÏ≤≠ÏôÄÎåÄÏùò Ï†úÏàòÏù¥Ìä∏Í∞Ä Ïïà ÏßÄÏÇ¨Î•º ÎÇôÎßàÏãúÏº∞Îã§‚ÄôÍ≥† ÎßêÌñàÎã§‚ÄùÍ≥†ÎèÑ ÌñàÎã§. Î¨∏ÎèôÏÑ± Í∏∞Ïûê theMoon@kmib.co.kr ÏÇ¨ÏßÑ=ÏµúÌòÑÍ∑ú Í∏∞Ïûê\n",
      "\u001b[1mQuestion: \u001b[0m\n",
      "ÍπÄÏî®Îäî Ïñ∏Ï†ú ÏûêÏã†Ïùò ÌéòÏù¥Ïä§Î∂Å Í≥ÑÏ†ï ‚ÄòSj Kim(ÎìúÎ£®ÌÇπ)‚ÄôÏùÑ ÌÜµÌï¥ ‚ÄúÍ∏∞Ï°¥ Îß§ÌÅ¨Î°ú Í∞ôÏùÄ Í≤ÉÏùÄ Ïù¥ÌãÄ Ï†ÑÎ∂ÄÌÑ∞ ÎßâÌòÄÏÑú Ïïà Îê† Í≤É‚ÄùÏù¥ÎùºÍ≥† Ïñ∏Í∏âÌñàÎÇò?\n",
      "\u001b[1mAnswer: \u001b[0m\n",
      "  Start: 202, End: 211\n",
      "\n",
      "\u001b[1mContext: \u001b[0m\n",
      "Í≤ΩÍ∏∞ÎèÑ ÏàòÏõê Í¥ëÍµêÏã†ÎèÑÏãú ÎÇ¥ ÏµúÎåÄ Í∑úÎ™®Ïùò Í∞úÎ∞úÏÇ¨ÏóÖÏù∏ 'ÏóêÏΩòÌûê(Econ hill¬∑Ï°∞Í∞êÎèÑ)' Ï°∞ÏÑ±ÏÇ¨ÏóÖÏù¥ Í≤∞Íµ≠ Î¨¥ÏÇ∞ÎêêÎã§. Í≤ΩÍ∏∞ÎèÑÏãúÍ≥µÏÇ¨Îäî 25Ïùº Ïò§Ï†Ñ ÏûÑÏãú Ïù¥ÏÇ¨ÌöåÎ•º Ïó¥Í≥† ÏãúÌñâÏÇ¨Ïù∏ ÏóêÏΩòÌûêÏùò ÏûêÏÇ∞Ïú†ÎèôÌôîÍ∏∞ÏóÖÏñ¥Ïùå(ABCP¬∑Î∂ÄÎèôÏÇ∞ Í¥ÄÎ†® ÏûêÏÇ∞ÏùÑ Îã¥Î≥¥Î°ú Î∞úÌñâÎêòÎäî Í∏∞ÏóÖÏñ¥Ïùå) 3700ÏñµÏõêÏùò ÎßåÍ∏∞Ïó∞Ïû•Ïóê ÎèôÏùòÌïòÏßÄ ÏïäÍ∏∞Î°ú ÏµúÏ¢Ö ÏùòÍ≤∞ÌñàÎã§. Ïù¥Ïóê Îî∞Îùº ÏóêÏΩòÌûêÏùÄ ÌÜ†ÏßÄÍ≥ÑÏïΩ Ìï¥ÏßÄ Îì± ÌååÏÇ∞Ï†àÏ∞®Î•º Î∞üÍ≤å ÎêúÎã§. Î∞ïÏàúÌò∏ ÎèÑÏãúÍ≥µÏÇ¨ ÌôçÎ≥¥Ï≤òÏû•ÏùÄ \"ÏãúÌñâÏÇ¨Ïù∏ ÏóêÏΩòÌûêÏ∏°Ïù¥ ÏûêÍ∏àÌôïÎ≥¥ Îì± ÏÇ¨ÏóÖÏ†ïÏÉÅÌôî Í≥ÑÌöçÏùÑ Î∞ùÌûàÏßÄ ÏïäÏïÑ Ïñ¥ÏùåÏùò ÎßåÍ∏∞Î•º Ïó∞Ïû•ÌïòÏßÄ ÏïäÍ∏∞Î°ú ÌñàÎã§\"Í≥† ÎßêÌñàÎã§. ÏóêÏΩòÌûêÏùÄ Í¥ëÍµêÏã†ÎèÑÏãú ÎÇ®Ï∏° 42Î≤à Íµ≠ÎèÑÎ≥Ä ÏÉÅÏóÖÏö©ÏßÄÏôÄ Ï£ºÏÉÅÎ≥µÌï©Ïö©ÏßÄ(C3, C4) 11Îßå7000Ïó¨„é°Ïóê ÏßÄÌïò 5Ï∏µ¬∑ÏßÄÏÉÅ 68Ï∏µÏùò Ï£ºÏÉÅÎ≥µÌï©Í±¥Î¨º 5Í∞ú ÎèôÏùÑ ÎπÑÎ°ØÌï¥ Î¨∏Ìôî¬∑Ïú†ÌÜµ¬∑ÏóÖÎ¨¥ Î≥µÌï©Îã®ÏßÄÎ•º Ï°∞ÏÑ±ÌïòÎäî ÏÇ¨ÏóÖÏù¥Îã§. ÌÜ†ÏßÄÎπÑ 7900ÏñµÏõêÏùÑ Ìè¨Ìï®Ìï¥ Ï¥ù ÏÇ¨ÏóÖÎπÑÍ∞Ä 2Ï°∞1000ÏñµÏõêÏóê Îã¨ÌïúÎã§. ÎåÄÏö∞Í±¥ÏÑ§¬∑ÏÇ∞ÏóÖÏùÄÌñâÏù¥ Ï§ëÏã¨Ïù¥ Îèº \u001b[1m\u001b[31m2009ÎÖÑ 3Ïõî\u001b[0m ÌäπÏàòÎ™©Ï†ÅÎ≤ïÏù∏(SPC) ÏóêÏΩòÌûê„àúÏùÑ ÏÑ§Î¶Ω, Î∂ÄÏßÄ ÏÜåÏú†Ï£ºÏù∏ Í≤ΩÍ∏∞ÎèÑÏãúÍ≥µÏÇ¨ÏôÄ 7900ÏñµÏõêÏóê ÌÜ†ÏßÄÎß§Îß§Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÎã§. Ïù¥ÌõÑ ÎèÑÏãúÍ≥µÏÇ¨Í∞Ä Ï†úÍ≥µÌïú ÌÜ†ÏßÄÏ§ëÎèÑÍ∏àÎ∞òÌôòÏ±ÑÍ∂åÏùÑ Îã¥Î≥¥Î°ú ÏÇ∞ÏóÖÏùÄÌñâÏóêÏÑú 3700ÏñµÏõêÏùÑ ÎåÄÏ∂úÎ∞õÏïÑ Ï§ëÎèÑÍ∏àÏùÑ ÎÉàÏßÄÎßå Î∂ÄÎèôÏÇ∞ Í≤ΩÍ∏∞Ïπ®Ï≤¥ Îì±Ïùò ÏòÅÌñ•ÏúºÎ°ú Ï∂îÍ∞Ä Ïû¨Ïõê ÌôïÎ≥¥Ïóê Ïñ¥Î†§ÏõÄÏùÑ Í≤™Ïñ¥ÏôîÎã§. ÏóêÏΩòÌûêÏ∏°ÏùÄ Ï†ÑÏ≤¥ 3Í∞ú Î∏îÎ°ù Í∞ÄÏö¥Îç∞ 1Í≥≥ÏùÑ Ïö∞ÏÑ† Í∞úÎ∞úÌïòÎäî Î∞©ÏïàÍ≥º Ìï†Î∂ÄÏù¥ÏûêÎ©¥Ï†ú , ÏÉÅÏóÖÏãúÏÑ§ ÎåÄÎ¨ºÎ≥ÄÏ†ú Îì±ÏùÑ ÏöîÍµ¨Ìïú Í≤ÉÏúºÎ°ú ÏïåÎ†§Ï°åÎã§. ÌïòÏßÄÎßå ÎèÑÏãúÍ≥µÏÇ¨Îäî Í≥ÑÏïΩÏõêÏπôÏóê Ïñ¥Í∏ãÎÇòÎ©∞ Îã§Î•∏ Îß§ÏàòÏûêÏôÄ ÌòïÌèâÏÑ± Î¨∏Ï†ú Îì±ÏùÑ Ïö∞Î†§Ìï¥ Î∞õÏïÑÎì§Ïù¥ÏßÄ ÏïäÏïòÎã§. Ïù¥ÏôÄ Í¥ÄÎ†® Í¥ëÍµêÏã†ÎèÑÏãú ÏûÖÏ£ºÎØºÎì§ÏùÄ Í≤ΩÍ∏∞ÎèÑÏôÄ Í≤ΩÍ∏∞ÎèÑÏãúÍ≥µÏÇ¨Ïóê ÏóêÏΩòÌûê ÏÇ¨ÏóÖ Î¨¥ÏÇ∞Ïóê Îî∞Î•∏ ÎåÄÏ±ÖÎßàÎ†®ÏùÑ Ï¥âÍµ¨ÌñàÎã§. Í¥ëÍµêÏã†ÎèÑÏãú ÏûÖÏ£ºÏûê Ï¥ùÏó∞Ìï©ÌöåÎäî \"ÏóêÏΩòÌûê ÏÇ¨ÏóÖÏùÄ Í≥µÏÇ¨Ïùò Í≥ºÎèÑÌïú ÏßÄÎ∂ÑÏ∞∏Ïó¨, ÏÇ¨Í∏∞ÏóÖÏùò Í≥ºÎã§Ìïú ÏöîÍµ¨, Î∂ÄÎèôÏÇ∞Í≤ΩÍ∏∞ ÏòàÏ∏°Ïã§Ìå® Îì±ÏúºÎ°ú Ïù¥ÎØ∏ ÏòàÍ≥†Îêú Ïû¨ÏïôÏù¥ÏóàÎã§\"Î©∞ \"ÏÇ¨ÏóÖ Î¨¥ÏÇ∞ÏùÄ Í≤ΩÍ∏∞ÎèÑÏôÄ Í≤ΩÍ∏∞ÎèÑÏãúÍ≥µÏÇ¨Ïùò Î¨¥Îä•Î†•Í≥º Î¨¥ÏÇ¨ÏïàÏùº ÌÉì\"Ïù¥ÎùºÍ≥† ÎπÑÌåêÌñàÎã§. Ï¥ùÏó∞Ìï©ÌöåÎäî \"ÏóêÏΩòÌûêÏù¥ Î¨¥ÏÇ∞Îèº Í≥µÏÇ¨Ï∏°Ïù¥ Ìï¥Îãπ Î∂ÄÏßÄÎ•º Ïò§ÌîºÏä§ÌÖî Ïö©ÏßÄÎ°ú Îß§Í∞ÅÌï† Í≤ΩÏö∞ Ï£ºÍ±∞Î∞ÄÎèÑÍ∞Ä ÎÜíÏïÑÏßÄÎäî Îì± Í¥ëÍµêÏã†ÎèÑÏãúÎäî Î™ÖÌíàÏã†ÎèÑÏãúÍ∞Ä ÏïÑÎãå Î≤†ÎìúÌÉÄÏö¥ÏúºÎ°ú Ï†ÑÎùΩÌï† Í≤É\"Ïù¥ÎùºÍ≥† Ïö∞Î†§ÌñàÎã§. Ïù¥Ïóê ÎåÄÌï¥ ÎèÑÏãúÍ≥µÏÇ¨ Í¥ÄÍ≥ÑÏûêÎäî \"ÌÉúÏä§ÌÅ¨Ìè¨Ïä§ÌåÄÏùÑ Íµ¨ÏÑ±Ìï¥ 3Í∞úÏõî Ïù¥ÎÇ¥Ïóê Ïû¨Í≥µÎ™® Í≥ÑÌöçÏùÑ Î∞úÌëúÌï† ÏòàÏ†ï\"Ïù¥ÎùºÎ©∞ \"Ï£ºÏÉÅÎ≥µÌï©, ÏÉÅÏóÖÏãúÏÑ§, Î∞±ÌôîÏ†ê Îì± Í∏∞Ï°¥ Í∞úÎ∞ú ÏΩòÏÖâÌä∏Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌï† Í≤É\"Ïù¥ÎùºÍ≥† ÎßêÌñàÎã§. Í≥µÏÇ¨Îäî Ìï¥Îãπ Î∂ÄÏßÄÏóê ÎåÄÌï¥ ÏßÄÏ£ºÍ≥µÎèôÏÇ¨ÏóÖ(Í≤ΩÍ∏∞ÎèÑÏãúÍ≥µÏÇ¨Í∞Ä ÌÜ†ÏßÄÎ•º Ï†úÍ≥µÌïòÍ≥† Í±¥ÏÑ§ÏÇ¨Îäî Í±¥ÏÑ§ÎπÑÎßå Î∂ÄÎã¥ÌïòÎäî Î∞©Ïãù)ÏùÑ Ï∂îÏßÑÌïòÍ±∞ÎÇò ÏïÑÏòà ÏùºÎ∞òÎß§Í∞ÅÌïòÎäî Î∞©Ïïà Îì±ÏùÑ Í≤ÄÌÜ† Ï§ëÏù¥Îã§.\n",
      "\u001b[1mQuestion: \u001b[0m\n",
      "ÏóêÏΩòÌûê(Ï£º)Îäî Ïñ∏Ï†ú ÏÑ§Î¶ΩÎêòÏóàÏñ¥?\n",
      "\u001b[1mAnswer: \u001b[0m\n",
      "  Start: 451, End: 459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "for idx in np.random.randint(0, len(train_contexts), size=(2,)):\n",
    "    txt = train_answers[idx][\"text\"]\n",
    "    context = train_contexts[idx].split(txt)\n",
    "    context.insert(1, termcolor.colored(txt, \"red\", attrs=[\"bold\"]))\n",
    "    answer_end = train_answers[idx]['answer_start'] + len(train_answers[idx]['text'])  # not included like python range\n",
    "    print(termcolor.colored(\"Context: \", attrs=[\"bold\"]))\n",
    "    print(\"\".join(context))\n",
    "    print(termcolor.colored(\"Question: \", attrs=[\"bold\"]))\n",
    "    print(train_questions[idx])\n",
    "    print(termcolor.colored(\"Answer: \", attrs=[\"bold\"]))\n",
    "    print(f\"  Start: {train_answers[idx]['answer_start']}, End: {answer_end}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for idx, (answer, context) in enumerate(zip(answers, contexts)):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two ‚Äì fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "            print(f\"type1: {idx}\")\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "            print(f\"type2: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer, ElectraTokenizerFast, ElectraForQuestionAnswering\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")  \n",
    "# Fast Î•º Ïç®Ïïº ._encodings ÏÜçÏÑ±Ïù¥ ÏÉùÍ∏¥Îã§. \n",
    "# ÏïàÏóêÎäî Encoding classÎ°ú Îêú Îç∞Ïù¥ÌÑ∞Í∞Ä listÎ°§ ÏûàÏùå\n",
    "# train_encodings._encodings[0]\n",
    "# Encoding(num_tokens=365, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = 5\n",
    "train_encodings = tokenizer(\n",
    "    train_contexts[:sample_len], train_questions[:sample_len], padding = \"max_length\",\n",
    "    max_length=512, truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_contexts[:sample_len], val_questions[:sample_len], padding = \"max_length\",\n",
    "    max_length=512, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))  # char_to_token: Î¨∏ÏûêÍ∞Ä Î™á Î≤àÏß∏ ÌÜ†ÌÅ∞Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers[:sample_len])\n",
    "add_token_positions(val_encodings, val_answers[:sample_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args_dict = {\n",
    "  \"task\": \"korquad\",\n",
    "  \"data_dir\": \"data\",\n",
    "  \"ckpt_dir\": \"ckpt\",\n",
    "  \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
    "  \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
    "  \"threads\": 4,\n",
    "  \"version_2_with_negative\": False,\n",
    "  \"null_score_diff_threshold\": 0.0,\n",
    "  \"max_seq_length\": 512,\n",
    "  \"doc_stride\": 128,\n",
    "  \"max_query_length\": 64,\n",
    "  \"max_answer_length\": 30,\n",
    "  \"n_best_size\": 20,\n",
    "  \"verbose_logging\": True,\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"evaluate_during_training\": True,\n",
    "  \"eval_all_checkpoints\": True,\n",
    "  \"save_optimizer\": False,\n",
    "  \"do_lower_case\": False,\n",
    "  \"do_train\": True,\n",
    "  \"do_eval\": True,\n",
    "  \"num_train_epochs\": 7,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"adam_epsilon\": 1e-8,\n",
    "  \"warmup_proportion\": 0,\n",
    "  \"max_steps\": -1,\n",
    "  \"max_grad_norm\": 1.0,\n",
    "  \"no_cuda\": False,\n",
    "  \"model_type\": \"koelectra-base-v3\",\n",
    "  \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "  \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "  \"seed\": 42,\n",
    "  \"train_batch_size\": 8,\n",
    "  \"eval_batch_size\": 32,\n",
    "  \"logging_steps\": 1000,\n",
    "  \"save_steps\": 1000,\n",
    "  \"learning_rate\": 5e-5\n",
    "}\n",
    "     \n",
    "args = ARGS(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForQuestionAnswering, ElectraConfig\n",
    "config = ElectraConfig.from_pretrained(args.model_name_or_path)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(args.model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs torch.Size([5, 512])\n",
      "Start Tokens torch.Size([5])\n",
      "End Tokens torch.Size([5])\n",
      "Loss tensor(6.0529, grad_fn=<DivBackward0>)\n",
      "Start Logits torch.Size([5, 512])\n",
      "End Logits torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs\", input_ids.size())\n",
    "print(\"Start Tokens\", start_positions.size())\n",
    "print(\"End Tokens\", end_positions.size())\n",
    "print(\"Loss\", outputs.loss)\n",
    "print(\"Start Logits\", outputs.start_logits.size())\n",
    "print(\"End Logits\", outputs.end_logits.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there more Efficient way???..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import squad_convert_examples_to_features\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 643.59it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = SquadV2Processor()\n",
    "examples = processor.get_train_examples(data_dir=data_path, filename=\"test.json\")  # examplesÏùÄ Î®ºÏ†Ä whitespace Í∏∞Î∞òÏúºÎ°ú ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌï®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "convert squad examples to features:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "convert squad examples to features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 84.76it/s][A\n",
      "\n",
      "add example index and unique id: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 369216.90it/s]\n"
     ]
    }
   ],
   "source": [
    "features, train_dataset = squad_convert_examples_to_features(\n",
    "    examples=examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=True,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 30\n"
     ]
    }
   ],
   "source": [
    "a = features[1]\n",
    "print(a.start_position, a.end_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 '\n",
      "2 Íµ≠Ï†ú\n",
      "3 Ï≤≠ÏÜåÎÖÑ\n",
      "4 ##Ìè¨\n",
      "5 ##Îüº\n",
      "6 '\n",
      "7 Ïù¥\n",
      "8 Ïó¥Î¶¨\n",
      "9 ##Îäî\n",
      "10 Îïå\n",
      "11 ##Îäî\n",
      "12 ?\n",
      "13 [SEP]\n",
      "14 ÌïúÍµ≠\n",
      "15 ##Ï≤≠\n",
      "16 ##ÏÜåÎÖÑÎã®\n",
      "17 ##Ï≤¥\n",
      "18 ##Ìòë\n",
      "19 ##Ïùò\n",
      "20 ##Ìöå\n",
      "21 ##ÏôÄ\n",
      "22 Ïó¨ÏÑ±\n",
      "23 ##Í∞ÄÏ°±\n",
      "24 ##Î∂Ä\n",
      "25 ##Îäî\n",
      "26 22\n",
      "27 ##Ïùº\n",
      "28 ##Î∂ÄÌÑ∞\n",
      "29 28\n",
      "30 ##Ïùº\n",
      "31 ##Íπå\n",
      "32 ##ÏßÄ\n",
      "33 ÏÑúÏö∏\n",
      "34 ##Í≥º\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(a.tokens[:35]):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args_dict = {\n",
    "  \"task\": \"korquad\",\n",
    "  \"data_dir\": \"data\",\n",
    "  \"ckpt_dir\": \"ckpt\",\n",
    "  \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
    "  \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
    "  \"threads\": 4,\n",
    "  \"version_2_with_negative\": False,\n",
    "  \"null_score_diff_threshold\": 0.0,\n",
    "  \"max_seq_length\": 512,\n",
    "  \"doc_stride\": 128,\n",
    "  \"max_query_length\": 64,\n",
    "  \"max_answer_length\": 30,\n",
    "  \"n_best_size\": 20,\n",
    "  \"verbose_logging\": True,\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"evaluate_during_training\": True,\n",
    "  \"eval_all_checkpoints\": True,\n",
    "  \"save_optimizer\": False,\n",
    "  \"do_lower_case\": False,\n",
    "  \"do_train\": True,\n",
    "  \"do_eval\": True,\n",
    "  \"num_train_epochs\": 7,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"adam_epsilon\": 1e-8,\n",
    "  \"warmup_proportion\": 0,\n",
    "  \"max_steps\": -1,\n",
    "  \"max_grad_norm\": 1.0,\n",
    "  \"no_cuda\": False,\n",
    "  \"model_type\": \"koelectra-base-v3\",\n",
    "  \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "  \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "  \"seed\": 42,\n",
    "  \"train_batch_size\": 8,\n",
    "  \"eval_batch_size\": 32,\n",
    "  \"logging_steps\": 1000,\n",
    "  \"save_steps\": 1000,\n",
    "  \"learning_rate\": 5e-5\n",
    "}\n",
    "     \n",
    "args = ARGS(**args_dict)\n",
    "\n",
    "def tolist(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForQuestionAnswering, ElectraConfig\n",
    "config = ElectraConfig.from_pretrained(args.model_name_or_path)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(args.model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.train_batch_size)\n",
    "t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
    ")\n",
    "\n",
    "global_step = 1\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1],\n",
    "        \"token_type_ids\": batch[2],\n",
    "        \"start_positions\": batch[3],\n",
    "        \"end_positions\": batch[4],\n",
    "    }\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=inputs[\"input_ids\"]\n",
    "attention_mask=inputs[\"attention_mask\"]\n",
    "token_type_ids=inputs[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model.electra.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "o.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_o = model.qa_outputs(o.last_hidden_state)\n",
    "fin_o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict tensor([374, 413, 331, 298,  64, 502, 295, 411])\n",
      "start answer tensor([  0,  43,   0, 103,  64,  87, 114, 308])\n",
      "start predict tensor([253, 189, 132, 260,   1, 374, 381,  45])\n",
      "start answer tensor([  0,  43,   0, 112,  84,  91, 116, 317])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(f\"start predict {outputs.start_logits.argmax(1)}\")\n",
    "print(f\"start answer {inputs['start_positions']}\")\n",
    "print(f\"start predict {outputs.end_logits.argmax(1)}\")\n",
    "print(f\"start answer {inputs['end_positions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**eval phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 659.09it/s]\n",
      "\n",
      "convert squad examples to features:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "convert squad examples to features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 86.09it/s][A\n",
      "\n",
      "add example index and unique id: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 308404.71it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_examples = processor.get_dev_examples(data_dir=data_path, filename=\"test.json\")  # examplesÏùÄ Î®ºÏ†Ä whitespace Í∏∞Î∞òÏúºÎ°ú ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌï®\n",
    "eval_features, eval_dataset = squad_convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fea in eval_features:\n",
    "#     fea.unique_id -= 1000000000\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for batch in eval_dataloader:\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "        example_indices = batch[3]\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        eval_feature = eval_features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        output = [tolist(o[i]) for o in outputs.values()]\n",
    "        start_logits, end_logits = output\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction_file = \"./predictions.json\"\n",
    "output_nbest_file = \"./nbest_predictions.json\"\n",
    "output_null_log_odds_file = \"./null_odds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compute_predictions_logits(\n",
    "    eval_examples,\n",
    "    eval_features,\n",
    "    all_results,\n",
    "    args.n_best_size,\n",
    "    args.max_answer_length,\n",
    "    args.do_lower_case,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    output_null_log_odds_file,\n",
    "    args.verbose_logging,\n",
    "    args.version_2_with_negative,\n",
    "    args.null_score_diff_threshold,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1_57059-1\n",
      "Answer: ÌïúÍµ≠Ï≤≠ÏÜåÎÖÑÎã®Ï≤¥ÌòëÏùòÌöåÏôÄ Ïó¨ÏÑ±Í∞ÄÏ°±Î∂Ä\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57060-1\n",
      "Answer: 22ÏùºÎ∂ÄÌÑ∞ 28Ïùº\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57061-1\n",
      "Answer: 'Ï≤≠ÏÜåÎÖÑÍ≥º Îâ¥ÎØ∏ÎîîÏñ¥'\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57062-1\n",
      "Answer: Í∏∞Ï°∞Í∞ïÏó∞ÏùÑ ÏãúÏûëÏúºÎ°ú Íµ≠Í∞ÄÎ≥Ñ Ï£ºÏ†úÍ¥ÄÎ†® ÏÇ¨Î°ÄÎ∞úÌëú, Í∑∏Î£π ÌÜ†Î°† Î∞è Ï†ÑÏ≤¥Ï¥ùÌöå, 'Ï≤≠ÏÜåÎÖÑÏÑ†Ïñ∏Î¨∏' ÏûëÏÑ± Î∞è Ï±ÑÌÉù Îì± Îã§ÏñëÌïú ÌîÑÎ°úÍ∑∏Îû®ÏùÑ Ïö¥ÏòÅÌïúÎã§.\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "m5_306705-1\n",
      "Answer: ÏÉêÎ¶¨\n",
      "Predict: Ïú°ÏÉÅ Î≥ºÎßÅ ÏñëÍ∂Å Î¶¨Îì¨Ï≤¥Ï°∞ ÏóêÏñ¥Î°úÎπÖ ÏÑ†ÏàòÍ∂åÎåÄÌöå(Ïù¥Ìïò ÏïÑÏú°ÎåÄ)'ÏóêÏÑúÎäî\n",
      "\n",
      "c1_151305-1\n",
      "Answer: Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151306-1\n",
      "Answer: ÏßÄÎÇúÎã¨ 28Ïùº\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151307-1\n",
      "Answer: Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨Ïãú\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151308-1\n",
      "Answer: ‚ÄòÏ§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ‚ÄôÍ≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà(ÏïΩ 170ÎßåÏõê)ÏùÑ ÏàòÏó¨\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151309-1\n",
      "Answer: Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151310-1\n",
      "Answer: Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§\n",
      "Predict: ÌöåÎ≥µÌï¥ ÏßÄÎÇú\n",
      "\n",
      "c1_151311-1\n",
      "Answer: ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤É\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_36509-1\n",
      "Answer: Ï°∞Îã¨Ï≤≠\n",
      "Predict: ÏÜåÏÜ° Í≤∞Í≥º Ïù∏Ïö©Î•†(Ï°∞Îã¨Ï≤≠ Ìå®ÏÜåÏú®)ÏùÄ ÌèâÍ∑† 86%Ïóê Îã¨ÌïúÎã§.Î∂ÄÏ†ïÎãπÏ†úÏ†ú\n",
      "\n",
      "c1_36510-1\n",
      "Answer: 2012ÎÖÑ\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36511-1\n",
      "Answer: ÏÜåÏÜ° Ï†úÏû¨ ÏûêÏ≤¥Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏóàÎã§Îäî ÏùòÎØ∏\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36512-1\n",
      "Answer: Í≥ÑÏïΩÏã¨ÏÇ¨ÌòëÏùòÌöåÏóêÏÑú ÎÇ¥Î∂Ä Ïã¨ÏùòÎ•º Í±∞Ï≥ê Î∂ÄÏ†ïÎãπÏóÖÏ≤¥Î°ú Îì±Î°ùÏùÑ ÌïòÍ≥† Ï†úÏû¨Î•º Ïã§Ïãú\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36513-1\n",
      "Answer: ÏßÄÎ∞©Í≥ÑÏïΩÎ≤ïÏù¥ Í∞úÏ†ïÎêòÎ©¥ÏÑú ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê Ï†úÏû¨Í∂åÌïúÏù¥ Ï°∞Îã¨Ï≤≠ÏúºÎ°ú Ïù¥Í¥ÄÎêú Í≤É\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_81296-1\n",
      "Answer: NASA\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81297-1\n",
      "Answer: 2004ÎÖÑ 1Ïõî\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81298-1\n",
      "Answer: ÌôîÏÑ±\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81299-1\n",
      "Answer: Ïö¥ÏÑùÏúºÎ°ú Î≥¥Ïù¥Îäî ÌäπÏù¥Ìïú ÏïîÏÑù\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81300-1\n",
      "Answer: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81301-1\n",
      "Answer: Ïò§ÌçºÌäúÎãàÌã∞Ïùò Ï†ÅÏô∏ÏÑ† Î∂ÑÍ¥ëÍ≥ÑÏù∏ Mini-TESÏùò Î∂ÑÏÑùÏóê Îî∞Î•¥Î©¥, Ïù¥ ÏïîÏÑùÏóêÏÑúÎäî ÌôîÏÑ± ÏïîÏÑùÏóêÏÑú ÎÇòÏò§Îäî Ï†ÑÌòïÏ†ÅÏù∏ Ïó¥ Ï†ÅÏô∏ÏÑ†Ïù¥ ÎÇòÏò§ÏßÄ ÏïäÏïòÍ∏∞ ÎïåÎ¨∏Ïóê\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81302-1\n",
      "Answer: Ïù¥ ÏïîÏÑù Í∑ºÏ≤òÏóê Î®∏Î¨ºÎ©¥ÏÑú Ïù¥Í≤ÉÏù¥ Ïö¥ÏÑùÏù∏ÏßÄÎ•º ÌôïÏã§ÌïòÍ≤å ÏïåÏïÑÎÇº Í≤É\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81303-1\n",
      "Answer: ÌëúÎ©¥Ïù¥ Ïö∏ÌâÅÎ∂àÌâÅÌïú ÌôàÏù¥ ÌååÏó¨ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(eval_examples, predictions.items()):\n",
    "    if i.qas_id == j[0]:\n",
    "        print(i.qas_id)\n",
    "        print(f\"Answer: {i.answers[0]['text']}\")\n",
    "        print(f\"Predict: {j[1]}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(i.qas_id, j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = squad_evaluate(eval_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 0.0),\n",
       "             ('f1', 5.556363636363637),\n",
       "             ('total', 25),\n",
       "             ('HasAns_exact', 0.0),\n",
       "             ('HasAns_f1', 5.556363636363637),\n",
       "             ('HasAns_total', 25),\n",
       "             ('best_exact', 0.0),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 5.556363636363637),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why there is 0 to predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_rev = {v: k for k, v in tokenizer.vocab.items()}\n",
    "tostring = lambda x: \" \".join(x).replace(\" ##\", \"\").replace(\"[PAD]\", \"\").strip()\n",
    "def show_original(inputs):\n",
    "    tokens = [vocab_rev[i.item()] for i in inputs[\"input_ids\"]]\n",
    "    s, e = inputs[\"start_positions\"].item(), inputs[\"end_positions\"].item()\n",
    "    print(s, e)\n",
    "    print(\"Answer: \", tostring(tokens[s:(e+1)]))\n",
    "    print(tostring(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "103 112\n",
      "Answer:  Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë\n",
      "[CLS] Ï§ëÍµ≠ÏóêÏÑú ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†ÏßÑ ÏÇ¨ÎûåÏùÄ ÎàÑÍµ¨Ïïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅ [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï§ëÍµ≠ÏóêÏÑú ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†ÏßÑ ÏÇ¨ÎûåÏùÄ ÎàÑÍµ¨Ïïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "114 116\n",
      "Answer:  ÏßÄÎÇúÎã¨ 28Ïùº\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Ï∂îÎùΩÌïòÎäî ÏïÑÏù¥Î•º Íµ¨ÌïòÍ≥† ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑÍ±¥ Ïñ∏Ï†úÏïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Ï∂îÎùΩÌïòÎäî ÏïÑÏù¥Î•º Íµ¨ÌïòÍ≥† ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑÍ±¥ Ïñ∏Ï†úÏïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "87 91\n",
      "Answer:  Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨Ïãú\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Í≥≥ÏùÄ Ïñ¥ÎîîÏïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Í≥≥ÏùÄ Ïñ¥ÎîîÏïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxes = [5, 6, 7, 8, 9, 10]\n",
    "for i, idx in enumerate(idxes):\n",
    "    batch = train_dataset[idx]\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1],\n",
    "        \"token_type_ids\": batch[2],\n",
    "        \"start_positions\": batch[3],\n",
    "        \"end_positions\": batch[4],\n",
    "    }\n",
    "    if i % 2 == 0:\n",
    "        print(\"----------------------------\"*3)\n",
    "    show_original(inputs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If over 512 tokens, it will generate sentences from the back like: `tokens[-512:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "from transformers import (\n",
    "    ElectraForQuestionAnswering, \n",
    "    ElectraConfig, \n",
    "    ElectraTokenizer,\n",
    "    AdamW,\n",
    "    squad_convert_examples_to_features,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"ko_nia_normal_squad_all.json\"\n",
    "val_file = \"ko_nia_clue0529_squad_all.json\"\n",
    "\n",
    "repo_path = Path().absolute().parent\n",
    "data_path = repo_path.parent / \"data\" / \"AIhub\" / \"QA\"\n",
    "ckpt_path = repo_path.parent / \"ckpt\"\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path.mkdir()\n",
    "else:\n",
    "    for x in ckpt_path.glob(\"*\"):\n",
    "        if x.is_dir():\n",
    "            x.rmdir()\n",
    "        else:\n",
    "            x.unlink()\n",
    "    ckpt_path.rmdir()\n",
    "    ckpt_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"test.json\"\n",
    "val_file = \"test.json\"\n",
    "args_dict = {\n",
    "    \"task\": \"AIHub_QA\",\n",
    "    \"data_path\": data_path,\n",
    "    \"ckpt_path\": ckpt_path,\n",
    "    \"train_file\": train_file,\n",
    "    \"val_file\": val_file,\n",
    "    \"cache_file\": \"{}_cache\",\n",
    "    \"random_seed\": 77,\n",
    "    \"threads\": 4,\n",
    "    \"version_2_with_negative\": False,\n",
    "    \"null_score_diff_threshold\": 0.0,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"doc_stride\": 128,\n",
    "    \"max_query_length\": 64,\n",
    "    \"max_answer_length\": 30,\n",
    "    \"n_best_size\": 20,\n",
    "    \"verbose_logging\": True,\n",
    "    \"do_lower_case\": False,\n",
    "    \"num_train_epochs\": 50,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"warmup_proportion\": 0,\n",
    "    \"model_type\": \"koelectra-base-v3\",\n",
    "    \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "    \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "    \"seed\": 42,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"output_prediction_file\": \"predictions_{}.json\",\n",
    "    \"output_nbest_file\": \"nbest_predictions_{}.json\",\n",
    "    \"output_null_log_odds_file\": \"null_odds_{}.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.config = ElectraConfig.from_pretrained(self.hparams.model_name_or_path)\n",
    "        self.model = ElectraForQuestionAnswering.from_pretrained(\n",
    "            self.hparams.model_name_or_path, \n",
    "            config=self.config\n",
    "        )\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.hparams.model_name_or_path)\n",
    "        # create dataset and cache it\n",
    "        self.create_dataset(state=\"train\")\n",
    "        self.create_dataset(state=\"val\")\n",
    "        self.eval_examples, self.eval_features = self.load_cache(state=\"val\", return_dataset=False)\n",
    "        # function\n",
    "        self.tolist = lambda x: x.detach().cpu().tolist()\n",
    "        \n",
    "    def create_dataset(self, state:str=\"train\"):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            state: train or val\n",
    "        \"\"\"\n",
    "        processor = SquadV2Processor()\n",
    "        if state == \"train\":\n",
    "            examples = processor.get_train_examples(\n",
    "                data_dir=self.hparams.data_path, \n",
    "                filename=self.hparams.train_file\n",
    "            )\n",
    "            is_training = True\n",
    "        elif state == \"val\":\n",
    "            examples = processor.get_dev_examples(\n",
    "                data_dir=self.hparams.data_path, \n",
    "                filename=self.hparams.val_file\n",
    "            )\n",
    "            is_training = False\n",
    "        else:\n",
    "            raise ValueError(\"state should be train or val\")\n",
    "            \n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_seq_length=self.hparams.max_seq_length,\n",
    "            doc_stride=self.hparams.doc_stride,\n",
    "            max_query_length=self.hparams.max_query_length,\n",
    "            is_training=is_training,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=self.hparams.threads,\n",
    "        )\n",
    "        # https://huggingface.co/transformers/_modules/transformers/data/processors/squad.html\n",
    "        # TODO: Cannot find the key unique_id\n",
    "        # BUG: must set argument of `trainer: num_sanity_val_steps=0` to avoid error.\n",
    "\n",
    "        cache = dict(dataset=dataset, examples=examples, features=features)\n",
    "        torch.save(cache, self.hparams.ckpt_path / self.hparams.cache_file.format(state))\n",
    "        print(f\"[INFO] cache file saved! {self.hparams.ckpt_path / self.hparams.cache_file.format(state)}\")\n",
    "\n",
    "    def load_cache(self, state:str=\"train\", return_dataset:bool=True):\n",
    "        cache = torch.load(self.hparams.ckpt_path / self.hparams.cache_file.format(state))\n",
    "        dataset, examples, features = cache[\"dataset\"], cache[\"examples\"], cache[\"features\"]\n",
    "\n",
    "        if return_dataset:\n",
    "            return dataset\n",
    "        else:\n",
    "            return examples, features\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs_ids, attention_mask, token_type_ids, start_positions, end_positions, *_ = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=inputs_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        return  {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs_ids, attention_mask, token_type_ids, example_indices, *_ = batch\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids=inputs_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=None,\n",
    "            end_positions=None\n",
    "        )\n",
    "        \n",
    "        batch_results = []\n",
    "        \n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            eval_feature = self.eval_features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            output = [self.tolist(o[i]) for o in outputs.values()]\n",
    "            start_logits, end_logits = output\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "            batch_results.append(result)\n",
    "            \n",
    "        return batch_results\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        all_results = []\n",
    "        for res in outputs:\n",
    "            all_results += res\n",
    "\n",
    "        predictions = compute_predictions_logits(\n",
    "            self.eval_examples,\n",
    "            self.eval_features,\n",
    "            all_results,\n",
    "            self.hparams.n_best_size,\n",
    "            self.hparams.max_answer_length,\n",
    "            self.hparams.do_lower_case,\n",
    "            self.hparams.ckpt_path / self.hparams.output_prediction_file.format(self.global_step),\n",
    "            self.hparams.ckpt_path / self.hparams.output_nbest_file.format(self.global_step),\n",
    "            self.hparams.ckpt_path / self.hparams.output_null_log_odds_file.format(self.global_step),\n",
    "            self.hparams.verbose_logging,\n",
    "            self.hparams.version_2_with_negative,\n",
    "            self.hparams.null_score_diff_threshold,\n",
    "            self.tokenizer,\n",
    "        )\n",
    "        results = squad_evaluate(self.eval_examples, predictions)\n",
    "        accuracy = results[\"exact\"]\n",
    "        f1 = results[\"f1\"]\n",
    "        self.log(\"accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"f1\", f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        t_total = self.total_steps()\n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "                \"weight_decay\": 0.0\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            params=optimizer_grouped_parameters, \n",
    "            lr=self.hparams.learning_rate, \n",
    "            eps=self.hparams.adam_epsilon\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer, \n",
    "            num_warmup_steps=int(t_total * self.hparams.warmup_proportion), \n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def create_dataloader(self, state:str=\"train\"):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            state: train or val\n",
    "        \"\"\"\n",
    "        if state == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = self.hparams.train_batch_size\n",
    "        elif state == \"val\":\n",
    "            shuffle = False\n",
    "            batch_size = self.hparams.eval_batch_size\n",
    "        else:\n",
    "            raise ValueError(\"state should be train or val\")\n",
    "        dataset = self.load_cache(state, return_dataset=True)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.hparams.threads\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(state=\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(state=\"val\")\n",
    "    \n",
    "    def total_steps(self):\n",
    "        r\"\"\"\n",
    "        source: https://github.com/PyTorchLightning/pytorch-lightning/issues/1038\n",
    "        \"\"\"\n",
    "        return len(self.train_dataloader()) * self.hparams.num_train_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args_dict):\n",
    "    print(\"[INFO] Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"[INFO] Seed:\", args_dict[\"random_seed\"])\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        filename=\"epoch{epoch}-f1{f1:.4f}\",\n",
    "        monitor=\"f1\",\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "    pl.seed_everything(args_dict[\"random_seed\"])\n",
    "    model = Model(**args_dict)\n",
    "    \n",
    "    print(\"[INFO] Start FineTuning\")\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[checkpoint_callback],\n",
    "        max_epochs=args_dict[\"num_train_epochs\"],\n",
    "        deterministic=torch.cuda.is_available(),\n",
    "        gpus=-1 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
