{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Tutorial\n",
    "\n",
    "This is a tutorial for me to learn how to use transformer with huggingface.\n",
    "\n",
    "# Reference: \n",
    "- https://huggingface.co/\n",
    "- https://huggingface.co/transformers/\n",
    "- https://github.com/huggingface/datasets\n",
    "- https://colab.research.google.com/drive/1IPkZo1Wd-DghIOK6gJpcb0Dv4_Gv2kXB?usp=sharing#scrollTo=ImupuGXDGq7b\n",
    "- https://github.com/monologg/KoELECTRA\n",
    "- https://github.com/monologg/KoELECTRA/blob/master/finetune/run_squad.py\n",
    "- Korean Sentence Splitter: https://github.com/hyunwoongko/kss\n",
    "\n",
    "# Installation\n",
    "\n",
    "```bash\n",
    "$ pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.6.0\n",
      "TorchText Version: 0.8.0a0+c851c3e\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"TorchText Version: {torchtext.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "161306 / 84759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "need to install sentencepiece\n",
    "\n",
    "```bash\n",
    "$ pip install sentencepiece\n",
    "$ pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use?\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "- ConversationalPipeline\n",
    "- FeatureExtractionPipeline\n",
    "- FillMaskPipeline\n",
    "- QuestionAnsweringPipeline\n",
    "- SummarizationPipeline\n",
    "- TextClassificationPipeline\n",
    "- TextGenerationPipeline\n",
    "- TokenClassificationPipeline\n",
    "- TranslationPipeline\n",
    "- ZeroShotClassificationPipeline\n",
    "- Text2TextGenerationPipeline\n",
    "- TableQuestionAnsweringPipeline\n",
    "\n",
    "function: `pipeline`\n",
    "\n",
    "- \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "- \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "- \"ner\": will return a TokenClassificationPipeline.\n",
    "- \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "- \"fill-mask\": will return a FillMaskPipeline.\n",
    "- \"summarization\": will return a SummarizationPipeline.\n",
    "- \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "- \"text2text-generation\": will return a Text2TextGenerationPipeline.\n",
    "- \"text-generation\": will return a TextGenerationPipeline.\n",
    "- \"zero-shot-classification:: will return a ZeroShotClassificationPipeline.\n",
    "- \"conversation\": will return a ConversationalPipeline.\n",
    "\n",
    "model will be automatically downloaded in `~/.cache/huggingface/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"We are very happy to show you the ü§ó Transformers library.\",\n",
    "    \"I'll go to Apple Store.\",\n",
    "    \"This model covers a lot area. But, I won't use it. Since it is too hard to use.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier(sentences)\n",
    "for res in results:\n",
    "    print(f\"label: {res['label']}, with score: {round(res['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Custom Dataset\n",
    "\n",
    "https://huggingface.co/transformers/custom_datasets.html#qa-squad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "- Ï†úÎ™©(title)\n",
    "- Î≥∏Î¨∏Ïùò Ïπ¥ÌÖåÍ≥†Î¶¨(source)\n",
    "- Î≥∏Î¨∏(context)\n",
    "- ÏßàÎ¨∏ Î≤àÌò∏(id)\n",
    "- Ïú°ÌïòÏõêÏπô(classtype)\n",
    "- ÏßàÎ¨∏(question)\n",
    "- Ï†ïÎãµÏùò ÏãúÏûëÏúÑÏπò(answer_start)\n",
    "- Ï†ïÎãµ(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_normal_squad_all.json\n",
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_clue0529_squad_all.json\n",
      "/home/simonjisu/code/data/AIhub/QA/ko_nia_noanswer_squad_all.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "repo_path = Path().absolute().parent\n",
    "data_path = repo_path.parent / \"data\" / \"AIhub\" / \"QA\"\n",
    "for p in data_path.glob(\"ko_nia*all.json\"):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Length of Tokenized Context: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:00<00:00, 765222.04it/s]\n"
     ]
    }
   ],
   "source": [
    "path = data_path / \"ko_nia_clue0529_squad_all_preprocessed.json\"\n",
    "with open(path, 'rb') as f:\n",
    "    squad_dict = json.load(f)\n",
    "x = []\n",
    "for group in tqdm(squad_dict[\"data\"], total=len(squad_dict[\"data\"]), desc=\"Getting Length of Tokenized Context\"):\n",
    "    for paragraph in group[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            qas_id = qa[\"id\"]\n",
    "            if qas_id == 'm6_415182-2':\n",
    "                x.append(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classtype': 'work_what',\n",
       "  'id': 'm6_415181-1',\n",
       "  'answers': [{'text': 'Ïä§ÌÉÄÌä∏ÏóÖÏùò', 'answer_start': 44}],\n",
       "  'question': 'ÎπÑÎπÑÌÅêÍ∞Ä 2017ÎÖÑ Î∂ÄÌÑ∞ Î¨¥ÏóáÏùò ÌõÑÏõêÏùÑ Ïù¥Ïñ¥Í∞ÄÍ≥† ÏûàÏñ¥?',\n",
       "  'clue': [{'clue_start': 66,\n",
       "    'clue_text': '2018ÎÖÑ Ï≤´ Ïä§ÌÉÄÌä∏ÏóÖ ÌõÑÏõêÏúºÎ°ú &apos;ÎØ∏ÎìúÎ†àÏù¥Ìä∏&apos;(P2P/ÏÜåÏÖú Í∏∞Î∞ò Ìà¨Ïûê Ïä§ÌÉÄÌä∏ÏóÖ)Ïùò ÏõåÌÅ¨ÏÉµÏóê Ï∞∏Ïó¨ÌñàÎã§'}]},\n",
       " {'classtype': 'work_what',\n",
       "  'id': 'm6_415181-2',\n",
       "  'answers': [{'text': 'Ïä§ÌÉÄÌä∏ÏóÖÏùò', 'answer_start': 44}],\n",
       "  'question': 'ÎπÑÎπÑÌÅêÍ∞Ä 2017ÎÖÑ Î∂ÄÌÑ∞ Ïñ¥ÎîîÏùò ÌõÑÏõêÏùÑ Ïù¥Ïñ¥Í∞ÄÍ≥† ÏûàÎåÄ?',\n",
       "  'clue': [{'clue_start': 66,\n",
       "    'clue_text': '2018ÎÖÑ Ï≤´ Ïä§ÌÉÄÌä∏ÏóÖ ÌõÑÏõêÏúºÎ°ú &apos;ÎØ∏ÎìúÎ†àÏù¥Ìä∏&apos;(P2P/ÏÜåÏÖú Í∏∞Î∞ò Ìà¨Ïûê Ïä§ÌÉÄÌä∏ÏóÖ)Ïùò ÏõåÌÅ¨ÏÉµÏóê Ï∞∏Ïó¨ÌñàÎã§'}]},\n",
       " {'classtype': 'work_who',\n",
       "  'id': 'm6_415182-1',\n",
       "  'answers': [{'text': 'Íµ≠ÎÇ¥ 1ÏúÑ ÏπòÌÇ® ÌîÑÎûúÏ∞®Ïù¥Ï¶à Í∏∞ÏóÖ ÎπÑÎπÑÌÅê(ÌöåÏû• Ïú§ÌôçÍ∑º)Îäî ', 'answer_start': 0}],\n",
       "  'question': 'ÎàÑÍ∞Ä 2018ÎÖÑ Ï≤´ Ïä§ÌÉÄÌä∏ÏóÖ ÌõÑÏõêÏúºÎ°ú &apos;ÎØ∏ÎìúÎ†àÏù¥Ìä∏&apos;(P2P/ÏÜåÏÖú Í∏∞Î∞ò Ìà¨Ïûê Ïä§ÌÉÄÌä∏ÏóÖ)Ïùò ÏõåÌÅ¨ÏÉµÏóê Ï∞∏Ïó¨ÌñàÏñ¥?',\n",
       "  'clue': [{'clue_start': 231,\n",
       "    'clue_text': 'BBQÎäî ÏÑúÏö∏ÎåÄÌïôÍµê Ï∞ΩÏóÖÎèôÏïÑÎ¶¨ ‚ÄòÎÅåÎ¶º&apos;, Ïï°Ìã∞ÎπÑÌã∞ ÏòàÏïΩÏÑúÎπÑÏä§ ‚ÄòÎç∞Ïñº&apos;, Ïù¥ÏÇ¨ Ïä§ÌÉÄÌä∏ÏóÖ ‚ÄòÏßêÏπ¥&apos;, Í∞ÄÏπò Î¨∏Ìôî ÌôïÏÇ∞ ÎÑ§Ìä∏ÏõåÌÅ¨ ‚ÄòVIRUS&apos;, Î∞òÎ†§ÎèôÎ¨º ÏÑúÎπÑÏä§ ‚ÄòÌéòÏò§Ìé´&apos;, ÌïúÎ≥µ ÌÅêÎ†àÏù¥ÏÖòÏÉµ ‚ÄòÌïòÌîåÎ¶¨&apos; Îì± Ïû†Ïû¨Î†• ÏûàÎäî Ïä§ÌÉÄÌä∏ÏóÖÏùÑ ÏàôÎ∞ïÏÑúÎπÑÏä§ Í∏∞ÏóÖ Ïó¨Í∏∞Ïñ¥ÎïåÏôÄ Ìï®Íªò Î∞úÍµ¥, ÏßÄÏÜçÏ†ÅÏù∏ ÌõÑÏõêÏùÑ Ïù¥Ïñ¥Í∞ÄÍ≥† ÏûàÎã§.'}]},\n",
       " {'classtype': 'work_who',\n",
       "  'id': 'm6_415182-2',\n",
       "  'answers': [{'text': 'Íµ≠ÎÇ¥ 1ÏúÑ ÏπòÌÇ® ÌîÑÎûúÏ∞®Ïù¥Ï¶à Í∏∞ÏóÖ ÎπÑÎπÑÌÅê(ÌöåÏû• Ïú§ÌôçÍ∑º)Îäî ', 'answer_start': 0}],\n",
       "  'question': '',\n",
       "  'clue': [{'clue_start': 231,\n",
       "    'clue_text': 'BBQÎäî ÏÑúÏö∏ÎåÄÌïôÍµê Ï∞ΩÏóÖÎèôÏïÑÎ¶¨ ‚ÄòÎÅåÎ¶º&apos;, Ïï°Ìã∞ÎπÑÌã∞ ÏòàÏïΩÏÑúÎπÑÏä§ ‚ÄòÎç∞Ïñº&apos;, Ïù¥ÏÇ¨ Ïä§ÌÉÄÌä∏ÏóÖ ‚ÄòÏßêÏπ¥&apos;, Í∞ÄÏπò Î¨∏Ìôî ÌôïÏÇ∞ ÎÑ§Ìä∏ÏõåÌÅ¨ ‚ÄòVIRUS&apos;, Î∞òÎ†§ÎèôÎ¨º ÏÑúÎπÑÏä§ ‚ÄòÌéòÏò§Ìé´&apos;, ÌïúÎ≥µ ÌÅêÎ†àÏù¥ÏÖòÏÉµ ‚ÄòÌïòÌîåÎ¶¨&apos; Îì± Ïû†Ïû¨Î†• ÏûàÎäî Ïä§ÌÉÄÌä∏ÏóÖÏùÑ ÏàôÎ∞ïÏÑúÎπÑÏä§ Í∏∞ÏóÖ Ïó¨Í∏∞Ïñ¥ÎïåÏôÄ Ìï®Íªò Î∞úÍµ¥, ÏßÄÏÜçÏ†ÅÏù∏ ÌõÑÏõêÏùÑ Ïù¥Ïñ¥Í∞ÄÍ≥† ÏûàÎã§.'}]}]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][\"paragraphs\"][0][\"qas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading datset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47314/47314 [00:00<00:00, 500836.12it/s]\n",
      "Reading datset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:00<00:00, 482808.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped id: m6_413880-2\n",
      "dropped id: m6_415182-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading datset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20030/20030 [00:00<00:00, 483338.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in data_path.glob(\"ko_nia*all.json\"):\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    missing_context_ids = []\n",
    "    missing_qas_ids = []\n",
    "    for i, group in tqdm(enumerate(squad_dict[\"data\"]), total=len(squad_dict[\"data\"]), desc=\"Reading datset\"):\n",
    "        for paragraph in group[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            if context == \"\":\n",
    "                missing_context_ids.append(i)\n",
    "                continue\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                if question_text == \"\":\n",
    "                    missing_qas_ids.append(qas_id)\n",
    "    for i in range(len(squad_dict[\"data\"])):\n",
    "        if i in missing_context_ids:\n",
    "            squad_dict[\"data\"].pop(i)\n",
    "            continue\n",
    "        qas = squad_dict[\"data\"][i][\"paragraphs\"][0][\"qas\"]\n",
    "        for j, qa in enumerate(qas):\n",
    "            if qa[\"id\"] in missing_qas_ids:\n",
    "                squad_dict[\"data\"][i][\"paragraphs\"][0][\"qas\"].pop(j)\n",
    "                print(f\"dropped id: {qa['id']}\")\n",
    "    p = data_path / (path.name.strip(\".json\") + \"_preprocessed.json\")\n",
    "    with open(p, \"w\") as f:\n",
    "        json.dump(squad_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m6_438965-1\n",
      "m6_438965-2\n"
     ]
    }
   ],
   "source": [
    "for q in qas:\n",
    "    print(q[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to see if we have over 512 tokens of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "def filter_dataset(args_dict):\n",
    "    data_path = args_dict[\"data_path\"]\n",
    "    model_name_or_path = args_dict[\"model_name_or_path\"]\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    length_data = {}\n",
    "    for path in data_path.glob(\"ko_nia*all.json\"):\n",
    "        if path.name == \"ko_nia_normal_squad_all.json\":\n",
    "            state = \"train\"\n",
    "        elif path.name == \"ko_nia_clue0529_squad_all.json\":\n",
    "            state = \"val\"\n",
    "        else:\n",
    "            continue\n",
    "        with open(path, 'rb') as f:\n",
    "            squad_dict = json.load(f)\n",
    "        length_of_texts = []\n",
    "        for group in tqdm(squad_dict[\"data\"], total=len(squad_dict[\"data\"]), desc=\"Getting Length of Tokenized Context\"):\n",
    "            for paragraph in group[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                length_of_texts.append(len(tokenizer.encode(context)))\n",
    "        length_of_texts = np.array(length_of_texts)\n",
    "        for token_length in [512, 1024]:\n",
    "            print(f\"Processing: {state}/{token_length}\")\n",
    "            filter_n = token_length if token_length == 512 else token_length*2\n",
    "            idx_over = np.arange(len(length_of_texts))[length_of_texts >= filter_n]\n",
    "            print(f\"count: # of over {token_length} is {len(idx_over)}/{len(length_of_texts)}, percentage: {len(idx_over)/len(length_of_texts)*100:.2f}%\")\n",
    "\n",
    "            choose_data = []\n",
    "            for i in tqdm(range(len(squad_dict[\"data\"])), total=len(squad_dict[\"data\"]), desc=f\"{state}, {token_length}\"):\n",
    "                if i in idx_over:\n",
    "                    continue\n",
    "                else:\n",
    "                    choose_data.append(squad_dict[\"data\"][i])\n",
    "            temp_data = dict(\n",
    "                creator=squad_dict[\"creator\"], \n",
    "                version=squad_dict[\"version\"], \n",
    "                data=choose_data\n",
    "            )\n",
    "            p = data_path / f\"AIhub_squad_{state}_{token_length}.json\"\n",
    "            with open(p, \"w\") as f:\n",
    "                json.dump(temp_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"data_path\": data_path\n",
    "    \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\"\n",
    "}\n",
    "filter_dataset(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 10\n",
    "processd_length = []\n",
    "for path in data_path.glob(\"*all.json\"):\n",
    "    if path.name == \"ko_nia_normal_squad_all.json\":\n",
    "        state = \"train\"\n",
    "    elif path.name == \"ko_nia_clue0529_squad_all.json\":\n",
    "        state = \"val\"\n",
    "    else:\n",
    "        continue\n",
    "    # read\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    total_examples = len(squad_dict[\"data\"])\n",
    "    k = len(squad_dict[\"data\"]) // n_split\n",
    "    processed = 0\n",
    "    \n",
    "    for i in range(n_split):\n",
    "        p = data_path / f\"{state}_{i}.json\"\n",
    "        temp_data = dict(\n",
    "            creator=squad_dict[\"creator\"], \n",
    "            version=squad_dict[\"version\"], \n",
    "            data=squad_dict[\"data\"][i:i+k]\n",
    "        )\n",
    "        processed += k\n",
    "        with open(p, \"w\") as f:\n",
    "            json.dump(temp_data, f)\n",
    "            \n",
    "    if processed < total_examples:\n",
    "        p = data_path / f\"{state}_{i+1}.json\"\n",
    "        temp_data = dict(\n",
    "            creator=squad_dict[\"creator\"], \n",
    "            version=squad_dict[\"version\"], \n",
    "            data=squad_dict[\"data\"][processed:]\n",
    "        )\n",
    "        with open(p, \"w\") as f:\n",
    "            json.dump(temp_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in tqdm(squad_dict[\"data\"], total=len(squad_dict[\"data\"]), desc=\"Reading Dataset\"):\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47314/47314 [00:00<00:00, 377770.56it/s]\n",
      "Reading Dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:00<00:00, 457291.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_file = \"ko_nia_normal_squad_all.json\"\n",
    "train_path = data_path / train_file\n",
    "val_file = \"ko_nia_clue0529_squad_all.json\"\n",
    "val_path = data_path / val_file\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad(train_path)\n",
    "val_contexts, val_questions, val_answers = read_squad(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243425 96663\n"
     ]
    }
   ],
   "source": [
    "print(len(train_contexts), len(val_contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[58567]Context: \u001b[0m\n",
      "‚ÄòÎØ∏Ïä§Ìã∞‚Äô(JTBC) ‚ÄòÎ¶¨ÌÑ¥‚Äô(SBS) ‚ÄòÏûëÏùÄ Ïã†Ïùò ÏïÑÏù¥Îì§‚Äô(OCN) ‚ÄòÎßàÎçî‚Äô(tvN). ÏµúÍ∑º Î∞©ÏÜ°Îêú Ïù¥ ÎìúÎùºÎßàÎì§Ïùò Í≥µÌÜµÏ†êÏùÄ ÏÑ∏ Í∞ÄÏßÄÎã§. ÎØ∏Ïä§ÌÑ∞Î¶¨Î•º Í∏∞Î∞òÏúºÎ°ú Ïä§ÌÜ†Î¶¨Í∞Ä Ï†ÑÍ∞úÎêúÎã§Îäî Í≤É, ÌÉÑÌÉÑÌïú Ïù¥ÏïºÍ∏∞ Íµ¨Ï°∞Î°ú Ìò∏ÌèâÏùÑ Î∞õÏïòÎã§Îäî Í≤É, Í∑πÎ≥∏ÏùÑ Ïì¥ ÏûëÍ∞ÄÎì§Ïùò Ï≤´ Ïû•Ìé∏ ÎìúÎùºÎßàÎùºÎäî Í≤ÉÏù¥Îã§. ‚ÄòÎØ∏Ïä§Ìã∞‚ÄôÎ•º Ïì¥ Ï†úÏù∏(Î≥∏Î™Ö ÍπÄÏû¨Ïù∏), ‚ÄòÎ¶¨ÌÑ¥‚ÄôÏùò ÏµúÍ≤ΩÎØ∏, ‚ÄòÏûëÏùÄ Ïã†Ïùò ÏïÑÏù¥Îì§‚Äô ÌïúÏö∞Î¶¨, ‚ÄòÎßàÎçî‚Äô Ï†ïÏÑúÍ≤Ω. ÎÑ§ ÏûëÍ∞Ä Î™®Îëê Ïã†Ïù∏Ïù¥Îã§. Í∞úÏó∞ÏÑ±Ïù¥ Îñ®Ïñ¥ÏßÄÍ±∞ÎÇò ÏÜçÎèÑ Ï°∞Ï†àÏùÑ Î™ª ÌïòÎ©¥ Í∏àÏÑ∏ ÌóàÏà†Ìï¥ÏßÄÍ∏∞ Ïâ¨Ïö¥ ÎØ∏Ïä§ÌÑ∞Î¶¨Î•º Ï∞®Ïö©Ìï¥ ÏûëÌíàÏùÑ Ïç® ÎÉàÎã§. ÏßúÏûÑÏÉà ÏûàÎäî Íµ¨ÏÑ±Í≥º Ïã†ÏÑ†Ìïú Ï†ÑÍ∞úÎ°ú Î∞©ÏÜ°Í∞ÄÏóê ÏÉàÎ°úÏö¥ ÌôúÎ†•ÏùÑ Î∂àÏñ¥ÎÑ£Í≥† ÏûàÎã§. ÎìúÎùºÎßàÎäî Í∞êÎèÖÏùò Ïó∞Ï∂ú, Î∞∞Ïö∞Ïùò Ïó∞Í∏∞ÎèÑ Ï§ëÏöîÌïòÏßÄÎßå ÏûëÍ∞ÄÏùò ÌïÑÎ†•Ïù¥ ÏûëÌíàÏùò ÏôÑÏÑ±ÎèÑÎ•º ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í≤∞Ï†ïÌïúÎã§. ÎìúÎùºÎßàÎ•º ‚ÄòÏûëÍ∞ÄÏùò ÏòàÏà†‚ÄôÏù¥ÎùºÍ≥† Î∂ÄÎ•¥Îäî Í±¥ Í∑∏ÎûòÏÑúÎã§. ÏûëÍ∞ÄÏùò Ïã§Î†•Ïóê Îî∞Îùº ÏûëÌíà ÏàòÏ§ÄÏù¥ Ï¢åÏö∞ÎêúÎã§. Ïù¥Îì§Ïù¥ Ïû•Ìé∏ ÎìúÎùºÎßà Í≤ΩÌóòÏù¥ ÏóÜÎäîÎç∞ÎèÑ ÏôÑÏÑ±ÎèÑ ÎÜíÏùÄ ÏûëÌíàÏùÑ ÎÇ¥ÎÜìÏùÑ Ïàò ÏûàÏóàÎçò ÎπÑÍ≤∞ÏùÄ Î≠òÍπå. ÏûëÍ∞ÄÎì§Ïùò Í≤ΩÎ†•ÏùÑ Î≥¥Î©¥ Ïñ¥Îäê Ï†ïÎèÑ ÎÇ©ÎìùÏù¥ Í∞ÑÎã§. Ï†úÏù∏ ÏûëÍ∞ÄÎäî ÍµêÏñë Î∞©ÏÜ° Î∂ÄÎ¨∏ÏóêÏÑú 20ÎÖÑ Í∞ÄÍπåÏù¥ ÏùºÌï¥ Ïò® Í≤ÉÏúºÎ°ú ÏïåÎ†§Ï°åÍ≥†, ÌïúÏö∞Î¶¨ ÏûëÍ∞ÄÎäî ÏãúÏÇ¨ ÌîÑÎ°úÍ∑∏Îû® ‚ÄòÍ∑∏Í≤ÉÏù¥ ÏïåÍ≥† Ïã∂Îã§‚Äô(SBS) ÏûëÍ∞Ä Ï∂úÏã†Ïù¥Îã§. ÏµúÍ≤ΩÎØ∏ ÏûëÍ∞ÄÎäî 2007ÎÖÑ Îã®ÎßâÍ∑π ‚ÄòÏïÑÍ∑Ä‚Äô(KBS)Î•º ÏçºÍ≥† 2015ÎÖÑ SBSÎ¨∏ÌôîÏû¨Îã® Í∑πÎ≥∏ Í≥µÎ™®ÏóêÏÑú ÏµúÏö∞ÏàòÏÉÅÏùÑ Î∞õÏúºÎ©∞ ÎëêÍ∞ÅÏùÑ ÎÇòÌÉÄÎÉàÎã§. Ï†ïÏÑúÍ≤Ω ÏûëÍ∞ÄÎäî Ï∂©Î¨¥Î°úÏóêÏÑú Î®ºÏ†Ä Ïù∏Ï†ïÎ∞õÏïòÎã§. ÌïúÍµ≠ÏòàÏà†Ï¢ÖÌï©ÌïôÍµê ÏòÅÏÉÅÏõê Ï∂úÏã†ÏúºÎ°ú ÏòÅÌôî ‚ÄòÏπúÏ†àÌïú Í∏àÏûêÏî®‚Äô(2005), ‚ÄòÏã∏Ïù¥Î≥¥Í∑∏ÏßÄÎßå Í¥úÏ∞ÆÏïÑ‚Äô(2006), ‚ÄòÎ∞ïÏ•ê‚Äô(2009), ‚ÄòÏïÑÍ∞ÄÏî®‚Äô(2016) Îì±Ïùò Í∞ÅÎ≥∏ÏùÑ ÏçºÎã§. ÎÑ§ ÏÇ¨Îûå Î™®Îëê Í∞ÅÏûêÏùò Î∂ÑÏïºÏóêÏÑú ÏûëÍ∞ÄÎ°úÏÑú Í≤ΩÎ†•ÏùÑ Ï∞®Í∑ºÏ∞®Í∑º ÏåìÏïÑÏò® ÏÖàÏù¥Îã§. Î™ÖÏÑ± ÎåÄÏã† ÏûëÌíàÏÑ±ÏùÑ ÏÑ†ÌÉùÌïú Î∞©ÏÜ°ÏÇ¨Ïùò ÏïàÎ™©ÎèÑ Ï§ëÏöîÌïòÍ≤å ÏûëÏö©ÌñàÎã§. Ïã†Ïù∏ ÏûëÍ∞ÄÎì§Ïù¥ ÌôúÏïΩÌïú Î∞©ÏÜ°ÏÇ¨Îì§ÏùÑ Î≥¥Î©¥ ÏºÄÏù¥Î∏îÏù¥ÎÇò Ï¢ÖÌï©Ìé∏ÏÑ±Ï±ÑÎÑêÏù¥ ÎßéÎã§. \u001b[1m\u001b[31mÏßÄÏÉÅÌåå Î∞©ÏÜ°Ïù¥ ÏãúÏ≤≠Î•†Ïóê Í∏âÍ∏âÌï¥ Í≥µÏãùÌôîÎêú ÎìúÎùºÎßàÎ•º ÎãµÏäµÌïòÎäîÎç∞ Î∞òÌï¥ ÏºÄÏù¥Î∏îÏù¥ÎÇò Ï¢ÖÌé∏ÏùÄ ÏãúÏ≤≠Î•† ÏÑ±Ï†ÅÏóê Îçú ÏñΩÎß§Ïù¥Îã§Î≥¥Îãà ÏÉàÎ°úÏö¥ ÏãúÎèÑÎ•º Ï£ºÏ†ÄÌïòÏßÄ ÏïäÎäîÎã§.\u001b[0m Ïù¥Îü∞ ÏÉÅÌô©Ïù¥ Í≥ÑÏÜçÎêòÎ©¥ÏÑú ‚ÄòÏ§ÄÎπÑÎêú‚Äô Ïã†Ïù∏ ÏûëÍ∞ÄÎì§Ïù¥ ÏßÄÏÉÅÌååÎ≥¥Îã§ ÏºÄÏù¥Î∏îÏù¥ÎÇò Ï¢ÖÌé∏ Ï™ΩÏùÑ ÏÑ†Ìò∏ÌïòÎäî Í≤ΩÌñ•ÎèÑ ÏûàÎã§. Ï†ïÏÑùÌù¨ ÎìúÎùºÎßà ÌèâÎ°†Í∞ÄÎäî ‚ÄúÏßúÏûÑÏÉà ÏûàÍ≤å Ï§ÄÎπÑÎ•º ÎßéÏù¥ Ìï¥ ÎÜìÏùÄ Ïã†Ïù∏ ÏûëÍ∞ÄÏ∏µÏù¥ ÎëêÌÑ∞ÏõåÏßÄÍ≥† ÏûàÎã§. ÌäπÌûà KBS Îã®ÎßâÍ∑π Îì±ÏùÑ ÌÜµÌï¥ ÌõàÎ†®Ïù¥ Ïûò Îêú ÏûëÍ∞ÄÎì§Ïù¥ ÎßéÏïÑÏ°åÎã§‚ÄùÎ©∞ ‚ÄúÏã†Ïù∏ ÏûëÍ∞ÄÎì§Ïù¥ ÏûêÏú†Î°≠Í≤å ÏûêÏã†Ïùò ÏûëÌíàÏùÑ Ìï† Ïàò ÏûàÎäî Í∏∞ÌöåÍ∞Ä ÎßéÏïÑÏßÄÎ©¥ÏÑú Ï¢ãÏùÄ ÏÑ±Í≥ºÎ°ú Ïù¥Ïñ¥ÏßÄÍ≥† ÏûàÎã§‚ÄùÍ≥† Î∂ÑÏÑùÌñàÎã§. Î¨∏ÏàòÏ†ï Í∏∞Ïûê thursday@kmib.co.kr\n",
      "\u001b[1m[58567]Question: \u001b[0m\n",
      "Ïã†Ïù∏ ÏûëÍ∞ÄÎì§Ïù¥ ÏßÄÏÉÅÌååÎ≥¥Îã§ ÏºÄÏù¥Î∏îÏù¥ÎÇò Ï¢ÖÌé∏ Ï™ΩÏùÑ ÏÑ†Ìò∏ÌïòÎäî Ïù¥Ïú†Í∞Ä Î≠êÏïº?\n",
      "\u001b[1m[58567]Answer: \u001b[0m\n",
      "  Start: 894, End: 974\n",
      "\n",
      "\u001b[1m[85009]Context: \u001b[0m\n",
      "ÌòÑÎåÄÏûêÎèôÏ∞®Í∑∏Î£π Í≥ÑÏó¥ Í±¥ÏÑ§ÌöåÏÇ¨Ïù∏ ÌòÑÎåÄÏó†ÏΩî(ÎåÄÌëú Ï°∞ÏúÑÍ±¥ ÏÇ¨Ïû•)Îäî Í≤ΩÏÉÅÎÇ®ÎèÑ ÏßÑÏ£º ÌèâÍ±∞4ÏßÄÍµ¨ 2Î∏îÎ°ùÏùò Í≥µÎèôÏ£ºÌÉù ÏãúÍ≥µÏÇ¨Î°ú ÏÑ†Ï†ïÎêêÎã§Í≥† \u001b[1m\u001b[31m12Ïùº\u001b[0m Î∞ùÌòîÎã§. ÌèâÍ±∞4ÏßÄÍµ¨ ÎèÑÏãúÍ∞úÎ∞úÏÇ¨ÏóÖÏùÄ ÌèâÍ±∞Îèô ÏùºÎåÄ 48Îßå„é°Î∂ÄÏßÄÏóê 3000Í∞ÄÍµ¨ Í∑úÎ™® ÏïÑÌååÌä∏Î•º Ï°∞ÏÑ±ÌïòÎäî Ï¥àÎåÄÌòï ÌîÑÎ°úÏ†ùÌä∏Îã§. ÌÜ†ÏßÄ ÏÜåÏú†ÏûêÎì§Ïù¥ Ï°∞Ìï©ÏùÑ ÏÑ§Î¶ΩÌï¥ ÌôòÏßÄ Î∞©ÏãùÏúºÎ°ú Ï∂îÏßÑÌïòÎäî ÎØºÍ∞ÑÎèÑÏãúÍ∞úÎ∞úÏÇ¨ÏóÖÏù¥Îã§. ÌòÑÎåÄÏó†ÏΩîÎäî Ïù¥ ÏßÄÏó≠ 2Î∏îÎ°ù 8Îßå8800„é° Î∂ÄÏßÄÏóê 1800Í∞ÄÍµ¨ Í∑úÎ™®Ïùò 'Ïó†ÏΩîÌÉÄÏö¥'ÏùÑ ÏßÑÏ£ºÏßÄÏó≠Ïóê Í≥µÍ∏âÌïúÎã§. Ïù¥Îäî ÌòÑÎåÄÏó†ÏΩîÏùò Îã®Ïùº Í≥µÍ∏â Í∑úÎ™®Î°ú ÏµúÎåÄÏπòÎã§. ÌòÑÎåÄÏó†ÏΩîÎäî ÏßÄÌïò 1Ï∏µ, ÏßÄÏÉÅ 30‚àº36Ï∏µÏùò 14Í∞ú Îèô Í∑úÎ™®Ïùò ÏÑ§Í≥ÑÏïàÏùÑ ÎßàÎ†®ÌïòÍ∏∞Î°ú Ìï¥Îã§. Îòê ÏòàÏà†ÎèÑÏãú ÌîÑÎùºÌïòÎ•º Ï£ºÏ†úÎ°ú Ìï¥ ÏïÑÌååÌä∏ Î∏åÎûúÎìúÎ•º 'Ïó†ÏΩîÌÉÄÏö¥ Îçî ÌîÑÎùºÌïò'Î°ú ÏßÄÏùÑ ÏòàÏ†ïÏù¥Îã§. ÌòÑÎåÄÏó†ÏΩîÎäî Ïò¨Ìï¥ ÏßÑÏ£º ÌèâÍ±∞4ÏßÄÍµ¨Î•º ÎπÑÎ°ØÌïú Ï∂©ÎÇ® ÎãπÏßÑ ÌòÑÎåÄÏ†úÏ≤†ÏßÅÏû•Ïù∏Ï£ºÌÉùÏ°∞Ìï© ÏÇ¨ÏóÖ Îì±ÏùÑ ÌÜµÌï¥ Î™®Îëê 3400Í∞ÄÍµ¨Ïùò ÏïÑÌååÌä∏Î•º Í≥µÍ∏âÌï† Í≥ÑÌöçÏù¥Îã§.\n",
      "\u001b[1m[85009]Question: \u001b[0m\n",
      "ÌòÑÎåÄÏûêÎèôÏ∞®Í∑∏Î£π Í≥ÑÏó¥ Í±¥ÏÑ§ÌöåÏÇ¨Ïù∏ ÌòÑÎåÄÏó†ÏΩîÎäî Ïñ∏Ï†ú Í≤ΩÎÇ® ÏßÑÏ£º ÌèâÍ±∞4ÏßÄÍµ¨ 2Î∏îÎ°ùÏùò Í≥µÎèôÏ£ºÌÉù ÏãúÍ≥µÏÇ¨Î°ú ÏÑ†Ï†ïÎêêÎã§Í≥† Î∞ùÌòîÎäîÍ∞Ä?\n",
      "\u001b[1m[85009]Answer: \u001b[0m\n",
      "  Start: 69, End: 72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import termcolor\n",
    "\n",
    "for idx in np.random.randint(0, len(val_contexts), size=(2,)):\n",
    "    txt = train_answers[idx][\"text\"]\n",
    "    context = train_contexts[idx].split(txt)\n",
    "    context.insert(1, termcolor.colored(txt, \"red\", attrs=[\"bold\"]))\n",
    "    answer_end = train_answers[idx]['answer_start'] + len(train_answers[idx]['text'])  # not included like python range\n",
    "    print(termcolor.colored(f\"[{idx}]Context: \", attrs=[\"bold\"]))\n",
    "    print(\"\".join(context))\n",
    "    print(termcolor.colored(f\"[{idx}]Question: \", attrs=[\"bold\"]))\n",
    "    print(train_questions[idx])\n",
    "    print(termcolor.colored(f\"[{idx}]Answer: \", attrs=[\"bold\"]))\n",
    "    print(f\"  Start: {train_answers[idx]['answer_start']}, End: {answer_end}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for idx, (answer, context) in enumerate(zip(answers, contexts)):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two ‚Äì fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "            print(f\"type1: {idx}\")\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "            print(f\"type2: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer, ElectraTokenizerFast, ElectraForQuestionAnswering\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")  \n",
    "# Fast Î•º Ïç®Ïïº ._encodings ÏÜçÏÑ±Ïù¥ ÏÉùÍ∏¥Îã§. \n",
    "# ÏïàÏóêÎäî Encoding classÎ°ú Îêú Îç∞Ïù¥ÌÑ∞Í∞Ä listÎ°§ ÏûàÏùå\n",
    "# train_encodings._encodings[0]\n",
    "# Encoding(num_tokens=365, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = 5\n",
    "train_encodings = tokenizer(\n",
    "    train_contexts[:sample_len], train_questions[:sample_len], padding = \"max_length\",\n",
    "    max_length=512, truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_contexts[:sample_len], val_questions[:sample_len], padding = \"max_length\",\n",
    "    max_length=512, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))  # char_to_token: Î¨∏ÏûêÍ∞Ä Î™á Î≤àÏß∏ ÌÜ†ÌÅ∞Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers[:sample_len])\n",
    "add_token_positions(val_encodings, val_answers[:sample_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args_dict = {\n",
    "  \"task\": \"korquad\",\n",
    "  \"data_dir\": \"data\",\n",
    "  \"ckpt_dir\": \"ckpt\",\n",
    "  \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
    "  \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
    "  \"threads\": 4,\n",
    "  \"version_2_with_negative\": False,\n",
    "  \"null_score_diff_threshold\": 0.0,\n",
    "  \"max_seq_length\": 512,\n",
    "  \"doc_stride\": 128,\n",
    "  \"max_query_length\": 64,\n",
    "  \"max_answer_length\": 30,\n",
    "  \"n_best_size\": 20,\n",
    "  \"verbose_logging\": True,\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"evaluate_during_training\": True,\n",
    "  \"eval_all_checkpoints\": True,\n",
    "  \"save_optimizer\": False,\n",
    "  \"do_lower_case\": False,\n",
    "  \"do_train\": True,\n",
    "  \"do_eval\": True,\n",
    "  \"num_train_epochs\": 7,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"adam_epsilon\": 1e-8,\n",
    "  \"warmup_proportion\": 0,\n",
    "  \"max_steps\": -1,\n",
    "  \"max_grad_norm\": 1.0,\n",
    "  \"no_cuda\": False,\n",
    "  \"model_type\": \"koelectra-base-v3\",\n",
    "  \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "  \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "  \"seed\": 42,\n",
    "  \"train_batch_size\": 8,\n",
    "  \"eval_batch_size\": 32,\n",
    "  \"logging_steps\": 1000,\n",
    "  \"save_steps\": 1000,\n",
    "  \"learning_rate\": 5e-5\n",
    "}\n",
    "     \n",
    "args = ARGS(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForQuestionAnswering, ElectraConfig\n",
    "config = ElectraConfig.from_pretrained(args.model_name_or_path)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(args.model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs torch.Size([5, 512])\n",
      "Start Tokens torch.Size([5])\n",
      "End Tokens torch.Size([5])\n",
      "Loss tensor(6.0529, grad_fn=<DivBackward0>)\n",
      "Start Logits torch.Size([5, 512])\n",
      "End Logits torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs\", input_ids.size())\n",
    "print(\"Start Tokens\", start_positions.size())\n",
    "print(\"End Tokens\", end_positions.size())\n",
    "print(\"Loss\", outputs.loss)\n",
    "print(\"Start Logits\", outputs.start_logits.size())\n",
    "print(\"End Logits\", outputs.end_logits.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there more Efficient way???..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import squad_convert_examples_to_features\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:31<00:00, 1088.61it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = SquadV2Processor()\n",
    "examples = processor.get_train_examples(data_dir=data_path, filename=\"ko_nia_normal_squad_all.json\")  # examplesÏùÄ Î®ºÏ†Ä whitespace Í∏∞Î∞òÏúºÎ°ú ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌï®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 84.96it/s]\n",
      "add example index and unique id: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 67216.41it/s]\n"
     ]
    }
   ],
   "source": [
    "features, train_dataset = squad_convert_examples_to_features(\n",
    "    examples=examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=True,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 30\n"
     ]
    }
   ],
   "source": [
    "a = features[1]\n",
    "print(a.start_position, a.end_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 '\n",
      "2 Íµ≠Ï†ú\n",
      "3 Ï≤≠ÏÜåÎÖÑ\n",
      "4 ##Ìè¨\n",
      "5 ##Îüº\n",
      "6 '\n",
      "7 Ïù¥\n",
      "8 Ïó¥Î¶¨\n",
      "9 ##Îäî\n",
      "10 Îïå\n",
      "11 ##Îäî\n",
      "12 ?\n",
      "13 [SEP]\n",
      "14 ÌïúÍµ≠\n",
      "15 ##Ï≤≠\n",
      "16 ##ÏÜåÎÖÑÎã®\n",
      "17 ##Ï≤¥\n",
      "18 ##Ìòë\n",
      "19 ##Ïùò\n",
      "20 ##Ìöå\n",
      "21 ##ÏôÄ\n",
      "22 Ïó¨ÏÑ±\n",
      "23 ##Í∞ÄÏ°±\n",
      "24 ##Î∂Ä\n",
      "25 ##Îäî\n",
      "26 22\n",
      "27 ##Ïùº\n",
      "28 ##Î∂ÄÌÑ∞\n",
      "29 28\n",
      "30 ##Ïùº\n",
      "31 ##Íπå\n",
      "32 ##ÏßÄ\n",
      "33 ÏÑúÏö∏\n",
      "34 ##Í≥º\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(a.tokens[:35]):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args_dict = {\n",
    "  \"task\": \"korquad\",\n",
    "  \"data_dir\": \"data\",\n",
    "  \"ckpt_dir\": \"ckpt\",\n",
    "  \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
    "  \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
    "  \"threads\": 4,\n",
    "  \"version_2_with_negative\": False,\n",
    "  \"null_score_diff_threshold\": 0.0,\n",
    "  \"max_seq_length\": 512,\n",
    "  \"doc_stride\": 128,\n",
    "  \"max_query_length\": 64,\n",
    "  \"max_answer_length\": 30,\n",
    "  \"n_best_size\": 20,\n",
    "  \"verbose_logging\": True,\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"evaluate_during_training\": True,\n",
    "  \"eval_all_checkpoints\": True,\n",
    "  \"save_optimizer\": False,\n",
    "  \"do_lower_case\": False,\n",
    "  \"do_train\": True,\n",
    "  \"do_eval\": True,\n",
    "  \"num_train_epochs\": 7,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"adam_epsilon\": 1e-8,\n",
    "  \"warmup_proportion\": 0,\n",
    "  \"max_steps\": -1,\n",
    "  \"max_grad_norm\": 1.0,\n",
    "  \"no_cuda\": False,\n",
    "  \"model_type\": \"koelectra-base-v3\",\n",
    "  \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "  \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "  \"seed\": 42,\n",
    "  \"train_batch_size\": 8,\n",
    "  \"eval_batch_size\": 32,\n",
    "  \"logging_steps\": 1000,\n",
    "  \"save_steps\": 1000,\n",
    "  \"learning_rate\": 5e-5\n",
    "}\n",
    "     \n",
    "args = ARGS(**args_dict)\n",
    "\n",
    "def tolist(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForQuestionAnswering, ElectraConfig\n",
    "config = ElectraConfig.from_pretrained(args.model_name_or_path)\n",
    "model = ElectraForQuestionAnswering.from_pretrained(args.model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.train_batch_size)\n",
    "t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
    ")\n",
    "\n",
    "global_step = 1\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1],\n",
    "        \"token_type_ids\": batch[2],\n",
    "        \"start_positions\": batch[3],\n",
    "        \"end_positions\": batch[4],\n",
    "    }\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=inputs[\"input_ids\"]\n",
    "attention_mask=inputs[\"attention_mask\"]\n",
    "token_type_ids=inputs[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model.electra.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "o.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_o = model.qa_outputs(o.last_hidden_state)\n",
    "fin_o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predict tensor([257,  92,  26, 461,  28,  28, 468,  86])\n",
      "start answer tensor([  0, 108,  21,  87,  18,  16, 114,  37])\n",
      "start predict tensor([243,  64,  56, 178, 109, 109,  21, 333])\n",
      "start answer tensor([  0, 119,  31,  91,  18,  16, 116,  43])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(f\"start predict {outputs.start_logits.argmax(1)}\")\n",
    "print(f\"start answer {inputs['start_positions']}\")\n",
    "print(f\"start predict {outputs.end_logits.argmax(1)}\")\n",
    "print(f\"start answer {inputs['end_positions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**eval phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import TruncationStrategy\n",
    "\n",
    "def _new_check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "    # if len(doc_spans) == 1:\n",
    "    # return True\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n",
    "        if position < doc_span[\"start\"]:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span[\"start\"]\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SquadFeatures, BatchEncoding, PreTrainedTokenizerBase\n",
    "MULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\n",
    "\n",
    "def squad_convert_example_to_features(\n",
    "    example, max_seq_length, doc_stride, max_query_length, padding_strategy, is_training\n",
    "):\n",
    "    features = []\n",
    "    if is_training and not example.is_impossible:\n",
    "        # Get start and end position\n",
    "        start_position = example.start_position\n",
    "        end_position = example.end_position\n",
    "\n",
    "        # If the answer cannot be found in the text, then skip this example.\n",
    "        actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
    "        cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
    "        if actual_text.find(cleaned_answer_text) == -1:\n",
    "            logger.warning(\"Could not find answer: '%s' vs. '%s'\", actual_text, cleaned_answer_text)\n",
    "            return []\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(example.doc_tokens):\n",
    "        orig_to_tok_index.append(len(all_doc_tokens))\n",
    "        if tokenizer.__class__.__name__ in [\n",
    "            \"RobertaTokenizer\",\n",
    "            \"LongformerTokenizer\",\n",
    "            \"BartTokenizer\",\n",
    "            \"RobertaTokenizerFast\",\n",
    "            \"LongformerTokenizerFast\",\n",
    "            \"BartTokenizerFast\",\n",
    "        ]:\n",
    "            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)\n",
    "        else:\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "        for sub_token in sub_tokens:\n",
    "            tok_to_orig_index.append(i)\n",
    "            all_doc_tokens.append(sub_token)\n",
    "\n",
    "    if is_training and not example.is_impossible:\n",
    "        tok_start_position = orig_to_tok_index[example.start_position]\n",
    "        if example.end_position < len(example.doc_tokens) - 1:\n",
    "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "        else:\n",
    "            tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "        (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text\n",
    "        )\n",
    "\n",
    "    spans = []\n",
    "\n",
    "    truncated_query = tokenizer.encode(\n",
    "        example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length\n",
    "    )\n",
    "\n",
    "    # Tokenizers who insert 2 SEP tokens in-between <context> & <question> need to have special handling\n",
    "    # in the way they compute mask of added tokens.\n",
    "    tokenizer_type = type(tokenizer).__name__.replace(\"Tokenizer\", \"\").lower()\n",
    "    sequence_added_tokens = (\n",
    "        tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1\n",
    "        if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET\n",
    "        else tokenizer.model_max_length - tokenizer.max_len_single_sentence\n",
    "    )\n",
    "    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair\n",
    "\n",
    "    span_doc_tokens = all_doc_tokens\n",
    "    while len(spans) * doc_stride < len(all_doc_tokens):\n",
    "\n",
    "        # Define the side we want to truncate / pad and the text/pair sorting\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            texts = truncated_query\n",
    "            pairs = span_doc_tokens\n",
    "            truncation = TruncationStrategy.ONLY_SECOND.value\n",
    "        else:\n",
    "            texts = span_doc_tokens\n",
    "            pairs = truncated_query\n",
    "            truncation = TruncationStrategy.ONLY_FIRST.value\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n",
    "            texts,\n",
    "            pairs,\n",
    "            truncation=truncation,\n",
    "            padding=padding_strategy,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=True,\n",
    "            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        paragraph_len = min(\n",
    "            len(all_doc_tokens) - len(spans) * doc_stride,\n",
    "            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
    "        )\n",
    "\n",
    "        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
    "            if tokenizer.padding_side == \"right\":\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
    "            else:\n",
    "                last_padding_id_position = (\n",
    "                    len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n",
    "                )\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1 :]\n",
    "\n",
    "        else:\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
    "\n",
    "        token_to_orig_map = {}\n",
    "        for i in range(paragraph_len):\n",
    "            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == \"right\" else i\n",
    "            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
    "\n",
    "        encoded_dict[\"paragraph_len\"] = paragraph_len\n",
    "        encoded_dict[\"tokens\"] = tokens\n",
    "        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
    "        encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
    "        encoded_dict[\"token_is_max_context\"] = {}\n",
    "        encoded_dict[\"start\"] = len(spans) * doc_stride\n",
    "        encoded_dict[\"length\"] = paragraph_len\n",
    "\n",
    "        spans.append(encoded_dict)\n",
    "\n",
    "        if \"overflowing_tokens\" not in encoded_dict or (\n",
    "            \"overflowing_tokens\" in encoded_dict and len(encoded_dict[\"overflowing_tokens\"]) == 0\n",
    "        ):\n",
    "            break\n",
    "        span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n",
    "\n",
    "    for doc_span_index in range(len(spans)):\n",
    "        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n",
    "            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n",
    "            index = (\n",
    "                j\n",
    "                if tokenizer.padding_side == \"left\"\n",
    "                else spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n",
    "            )\n",
    "            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n",
    "\n",
    "    for span in spans:\n",
    "        # Identify the position of the CLS token\n",
    "        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "\n",
    "        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "        # Original TF implem also keep the classification token (set to 0)\n",
    "        p_mask = np.ones_like(span[\"token_type_ids\"])\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            p_mask[len(truncated_query) + sequence_added_tokens :] = 0\n",
    "        else:\n",
    "            p_mask[-len(span[\"tokens\"]) : -(len(truncated_query) + sequence_added_tokens)] = 0\n",
    "\n",
    "        pad_token_indices = np.where(span[\"input_ids\"] == tokenizer.pad_token_id)\n",
    "        special_token_indices = np.asarray(\n",
    "            tokenizer.get_special_tokens_mask(span[\"input_ids\"], already_has_special_tokens=True)\n",
    "        ).nonzero()\n",
    "\n",
    "        p_mask[pad_token_indices] = 1\n",
    "        p_mask[special_token_indices] = 1\n",
    "\n",
    "        # Set the cls index to 0: the CLS index can be used for impossible answers\n",
    "        p_mask[cls_index] = 0\n",
    "\n",
    "        span_is_impossible = example.is_impossible\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "        if is_training and not span_is_impossible:\n",
    "            # For training, if our document chunk does not contain an annotation\n",
    "            # we throw it out, since there is nothing to predict.\n",
    "            doc_start = span[\"start\"]\n",
    "            doc_end = span[\"start\"] + span[\"length\"] - 1\n",
    "            out_of_span = False\n",
    "\n",
    "            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
    "                out_of_span = True\n",
    "\n",
    "            if out_of_span:\n",
    "                start_position = cls_index\n",
    "                end_position = cls_index\n",
    "                span_is_impossible = True\n",
    "            else:\n",
    "                if tokenizer.padding_side == \"left\":\n",
    "                    doc_offset = 0\n",
    "                else:\n",
    "                    doc_offset = len(truncated_query) + sequence_added_tokens\n",
    "\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "        features.append(\n",
    "            SquadFeatures(\n",
    "                span[\"input_ids\"],\n",
    "                span[\"attention_mask\"],\n",
    "                span[\"token_type_ids\"],\n",
    "                cls_index,\n",
    "                p_mask.tolist(),\n",
    "                example_index=0,  # Can not set unique_id and example_index here. They will be set after multiple processing.\n",
    "                unique_id=0,\n",
    "                paragraph_len=span[\"paragraph_len\"],\n",
    "                token_is_max_context=span[\"token_is_max_context\"],\n",
    "                tokens=span[\"tokens\"],\n",
    "                token_to_orig_map=span[\"token_to_orig_map\"],\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=span_is_impossible,\n",
    "                qas_id=example.qas_id,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34500/34500 [00:28<00:00, 1216.19it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_examples = processor.get_dev_examples(data_dir=data_path, filename=\"ko_nia_clue0529_squad_all_preprocessed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 37748/96661 [07:43<12:02, 81.53it/s]  \n"
     ]
    }
   ],
   "source": [
    "max_seq_length=512\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "padding_strategy=\"max_length\"\n",
    "is_training=False\n",
    "for i, ex in tqdm(enumerate(eval_examples), total=len(eval_examples)):\n",
    "    try:\n",
    "        f = squad_convert_example_to_features(ex, max_seq_length, doc_stride, max_query_length, padding_strategy, is_training)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-270-91a9a5cc3ac0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-270-91a9a5cc3ac0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for a in ex\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for a in ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-dc272b6dcdc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdoc_stride\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated_query\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_pair_added_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m         )\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                     )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "\n",
    "features = []\n",
    "if is_training and not example.is_impossible:\n",
    "    # Get start and end position\n",
    "    start_position = example.start_position\n",
    "    end_position = example.end_position\n",
    "\n",
    "    # If the answer cannot be found in the text, then skip this example.\n",
    "    actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
    "    cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
    "    if actual_text.find(cleaned_answer_text) == -1:\n",
    "        logger.warning(\"Could not find answer: '%s' vs. '%s'\", actual_text, cleaned_answer_text)\n",
    "\n",
    "tok_to_orig_index = []\n",
    "orig_to_tok_index = []\n",
    "all_doc_tokens = []\n",
    "for (i, token) in enumerate(example.doc_tokens):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    if tokenizer.__class__.__name__ in [\n",
    "        \"RobertaTokenizer\",\n",
    "        \"LongformerTokenizer\",\n",
    "        \"BartTokenizer\",\n",
    "        \"RobertaTokenizerFast\",\n",
    "        \"LongformerTokenizerFast\",\n",
    "        \"BartTokenizerFast\",\n",
    "    ]:\n",
    "        sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)\n",
    "    else:\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "\n",
    "if is_training and not example.is_impossible:\n",
    "    tok_start_position = orig_to_tok_index[example.start_position]\n",
    "    if example.end_position < len(example.doc_tokens) - 1:\n",
    "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "    else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "    (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "        all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text\n",
    "    )\n",
    "\n",
    "spans = []\n",
    "\n",
    "truncated_query = tokenizer.encode(\n",
    "    example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length\n",
    ")\n",
    "\n",
    "# Tokenizers who insert 2 SEP tokens in-between <context> & <question> need to have special handling\n",
    "# in the way they compute mask of added tokens.\n",
    "tokenizer_type = type(tokenizer).__name__.replace(\"Tokenizer\", \"\").lower()\n",
    "sequence_added_tokens = (\n",
    "    tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1\n",
    "    if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET\n",
    "    else tokenizer.model_max_length - tokenizer.max_len_single_sentence\n",
    ")\n",
    "sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair\n",
    "\n",
    "span_doc_tokens = all_doc_tokens\n",
    "while len(spans) * doc_stride < len(all_doc_tokens):\n",
    "\n",
    "    # Define the side we want to truncate / pad and the text/pair sorting\n",
    "    if tokenizer.padding_side == \"right\":\n",
    "        texts = truncated_query\n",
    "        pairs = span_doc_tokens\n",
    "        truncation = TruncationStrategy.ONLY_SECOND.value\n",
    "    else:\n",
    "        texts = span_doc_tokens\n",
    "        pairs = truncated_query\n",
    "        truncation = TruncationStrategy.ONLY_FIRST.value\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n",
    "        texts,\n",
    "        pairs,\n",
    "        truncation=truncation,\n",
    "        padding=padding_strategy,\n",
    "        max_length=max_seq_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "\n",
    "    paragraph_len = min(\n",
    "        len(all_doc_tokens) - len(spans) * doc_stride,\n",
    "        max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
    "        else:\n",
    "            last_padding_id_position = (\n",
    "                len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n",
    "            )\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1 :]\n",
    "\n",
    "    else:\n",
    "        non_padded_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
    "\n",
    "    token_to_orig_map = {}\n",
    "    for i in range(paragraph_len):\n",
    "        index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == \"right\" else i\n",
    "        token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
    "\n",
    "    encoded_dict[\"paragraph_len\"] = paragraph_len\n",
    "    encoded_dict[\"tokens\"] = tokens\n",
    "    encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
    "    encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
    "    encoded_dict[\"token_is_max_context\"] = {}\n",
    "    encoded_dict[\"start\"] = len(spans) * doc_stride\n",
    "    encoded_dict[\"length\"] = paragraph_len\n",
    "\n",
    "    spans.append(encoded_dict)\n",
    "\n",
    "    if \"overflowing_tokens\" not in encoded_dict or (\n",
    "        \"overflowing_tokens\" in encoded_dict and len(encoded_dict[\"overflowing_tokens\"]) == 0\n",
    "    ):\n",
    "        break\n",
    "    span_doc_tokens = encoded_dict[\"overflowing_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\n",
    "    example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-4828c5f54d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdoc_stride\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated_query\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_pair_added_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m         )\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                     )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n",
    "    texts,\n",
    "    pairs,\n",
    "    truncation=truncation,\n",
    "    padding=padding_strategy,\n",
    "    max_length=max_seq_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
    "    return_token_type_ids=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-8c2d91e44180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msquad_convert_example_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-b3d6483e6811>\u001b[0m in \u001b[0;36msquad_convert_example_to_features\u001b[0;34m(example, max_seq_length, doc_stride, max_query_length, padding_strategy, is_training)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdoc_stride\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated_query\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_pair_added_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m         )\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                     )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "squad_convert_example_to_features(ex, 512, 128, 64, \"max_length\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in examples:\n",
    "    if \"‚Äò\" in e.context_text:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_features, eval_dataset = squad_convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fea in eval_features:\n",
    "#     fea.unique_id -= 1000000000\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 1000000000 / 0\n",
      " c: 0 / p: -1 / e: 0\n",
      "after: 1000000000 / 0\n",
      "\n",
      "before: 1000000001 / 1\n",
      " c: 1 / p: 0 / e: 1\n",
      "after: 1000000001 / 1\n",
      "\n",
      "before: 1000000002 / 2\n",
      " c: 2 / p: 1 / e: 2\n",
      "after: 1000000002 / 2\n",
      "\n",
      "before: 1000000003 / 3\n",
      " c: 3 / p: 2 / e: 3\n",
      "after: 1000000003 / 3\n",
      "\n",
      "before: 1000000004 / 4\n",
      " c: 4 / p: 3 / e: 4\n",
      "after: 1000000004 / 4\n",
      "\n",
      "before: 1000000005 / 5\n",
      " c: 5 / p: 4 / e: 5\n",
      "after: 1000000005 / 5\n",
      "\n",
      "before: 1000000006 / 5\n",
      " c: 5 / p: 5 / e: 6\n",
      "after: 1000000006 / 5\n",
      "\n",
      "before: 1000000007 / 6\n",
      " c: 6 / p: 5 / e: 6\n",
      "after: 1000000007 / 6\n",
      "\n",
      "before: 1000000008 / 6\n",
      " c: 6 / p: 6 / e: 7\n",
      "after: 1000000008 / 6\n",
      "\n",
      "before: 1000000009 / 7\n",
      " c: 7 / p: 6 / e: 7\n",
      "after: 1000000009 / 7\n",
      "\n",
      "before: 1000000010 / 7\n",
      " c: 7 / p: 7 / e: 8\n",
      "after: 1000000010 / 7\n",
      "\n",
      "before: 1000000011 / 8\n",
      " c: 8 / p: 7 / e: 8\n",
      "after: 1000000011 / 8\n",
      "\n",
      "before: 1000000012 / 8\n",
      " c: 8 / p: 8 / e: 9\n",
      "after: 1000000012 / 8\n",
      "\n",
      "before: 1000000013 / 9\n",
      " c: 9 / p: 8 / e: 9\n",
      "after: 1000000013 / 9\n",
      "\n",
      "before: 1000000014 / 9\n",
      " c: 9 / p: 9 / e: 10\n",
      "after: 1000000014 / 9\n",
      "\n",
      "before: 1000000015 / 10\n",
      " c: 10 / p: 9 / e: 10\n",
      "after: 1000000015 / 10\n",
      "\n",
      "before: 1000000016 / 10\n",
      " c: 10 / p: 10 / e: 11\n",
      "after: 1000000016 / 10\n",
      "\n",
      "before: 1000000017 / 11\n",
      " c: 11 / p: 10 / e: 11\n",
      "after: 1000000017 / 11\n",
      "\n",
      "before: 1000000018 / 11\n",
      " c: 11 / p: 11 / e: 12\n",
      "after: 1000000018 / 11\n",
      "\n",
      "before: 1000000019 / 12\n",
      " c: 12 / p: 11 / e: 12\n",
      "after: 1000000019 / 12\n",
      "\n",
      "before: 1000000020 / 13\n",
      " c: 13 / p: 12 / e: 13\n",
      "after: 1000000020 / 13\n",
      "\n",
      "before: 1000000021 / 14\n",
      " c: 14 / p: 13 / e: 14\n",
      "after: 1000000021 / 14\n",
      "\n",
      "before: 1000000022 / 15\n",
      " c: 15 / p: 14 / e: 15\n",
      "after: 1000000022 / 15\n",
      "\n",
      "before: 1000000023 / 16\n",
      " c: 16 / p: 15 / e: 16\n",
      "after: 1000000023 / 16\n",
      "\n",
      "before: 1000000024 / 17\n",
      " c: 17 / p: 16 / e: 17\n",
      "after: 1000000024 / 17\n",
      "\n",
      "before: 1000000025 / 18\n",
      " c: 18 / p: 17 / e: 18\n",
      "after: 1000000025 / 18\n",
      "\n",
      "before: 1000000026 / 19\n",
      " c: 19 / p: 18 / e: 19\n",
      "after: 1000000026 / 19\n",
      "\n",
      "before: 1000000027 / 20\n",
      " c: 20 / p: 19 / e: 20\n",
      "after: 1000000027 / 20\n",
      "\n",
      "before: 1000000028 / 21\n",
      " c: 21 / p: 20 / e: 21\n",
      "after: 1000000028 / 21\n",
      "\n",
      "before: 1000000029 / 22\n",
      " c: 22 / p: 21 / e: 22\n",
      "after: 1000000029 / 22\n",
      "\n",
      "before: 1000000030 / 23\n",
      " c: 23 / p: 22 / e: 23\n",
      "after: 1000000030 / 23\n",
      "\n",
      "before: 1000000031 / 24\n",
      " c: 24 / p: 23 / e: 24\n",
      "after: 1000000031 / 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = deepcopy(eval_features)\n",
    "example_index = 0\n",
    "unique_id = 1000000000\n",
    "previous_example_index = -1 \n",
    "for fea in features:\n",
    "    print(f\"before: {fea.unique_id} / {fea.example_index}\")\n",
    "    fea.unique_id = unique_id\n",
    "    unique_id += 1\n",
    "    \n",
    "    current_example_index = fea.example_index\n",
    "    print(f\" c: {current_example_index} / p: {previous_example_index} / e: {example_index}\")\n",
    "    if previous_example_index == current_example_index:\n",
    "        fea.example_index = previous_example_index\n",
    "    else:\n",
    "        previous_example_index = fea.example_index\n",
    "        fea.example_index = example_index\n",
    "        example_index += 1\n",
    "    print(f\"after: {fea.unique_id} / {fea.example_index}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 1000000000 / 0\n",
      " c: 0 / p: -1 / e: 25\n",
      "after: 1000000032 / 25\n",
      "\n",
      "before: 1000000001 / 1\n",
      " c: 1 / p: 0 / e: 26\n",
      "after: 1000000033 / 26\n",
      "\n",
      "before: 1000000002 / 2\n",
      " c: 2 / p: 1 / e: 27\n",
      "after: 1000000034 / 27\n",
      "\n",
      "before: 1000000003 / 3\n",
      " c: 3 / p: 2 / e: 28\n",
      "after: 1000000035 / 28\n",
      "\n",
      "before: 1000000004 / 4\n",
      " c: 4 / p: 3 / e: 29\n",
      "after: 1000000036 / 29\n",
      "\n",
      "before: 1000000005 / 5\n",
      " c: 5 / p: 4 / e: 30\n",
      "after: 1000000037 / 30\n",
      "\n",
      "before: 1000000006 / 5\n",
      " c: 5 / p: 5 / e: 31\n",
      "after: 1000000038 / 5\n",
      "\n",
      "before: 1000000007 / 6\n",
      " c: 6 / p: 5 / e: 31\n",
      "after: 1000000039 / 31\n",
      "\n",
      "before: 1000000008 / 6\n",
      " c: 6 / p: 6 / e: 32\n",
      "after: 1000000040 / 6\n",
      "\n",
      "before: 1000000009 / 7\n",
      " c: 7 / p: 6 / e: 32\n",
      "after: 1000000041 / 32\n",
      "\n",
      "before: 1000000010 / 7\n",
      " c: 7 / p: 7 / e: 33\n",
      "after: 1000000042 / 7\n",
      "\n",
      "before: 1000000011 / 8\n",
      " c: 8 / p: 7 / e: 33\n",
      "after: 1000000043 / 33\n",
      "\n",
      "before: 1000000012 / 8\n",
      " c: 8 / p: 8 / e: 34\n",
      "after: 1000000044 / 8\n",
      "\n",
      "before: 1000000013 / 9\n",
      " c: 9 / p: 8 / e: 34\n",
      "after: 1000000045 / 34\n",
      "\n",
      "before: 1000000014 / 9\n",
      " c: 9 / p: 9 / e: 35\n",
      "after: 1000000046 / 9\n",
      "\n",
      "before: 1000000015 / 10\n",
      " c: 10 / p: 9 / e: 35\n",
      "after: 1000000047 / 35\n",
      "\n",
      "before: 1000000016 / 10\n",
      " c: 10 / p: 10 / e: 36\n",
      "after: 1000000048 / 10\n",
      "\n",
      "before: 1000000017 / 11\n",
      " c: 11 / p: 10 / e: 36\n",
      "after: 1000000049 / 36\n",
      "\n",
      "before: 1000000018 / 11\n",
      " c: 11 / p: 11 / e: 37\n",
      "after: 1000000050 / 11\n",
      "\n",
      "before: 1000000019 / 12\n",
      " c: 12 / p: 11 / e: 37\n",
      "after: 1000000051 / 37\n",
      "\n",
      "before: 1000000020 / 13\n",
      " c: 13 / p: 12 / e: 38\n",
      "after: 1000000052 / 38\n",
      "\n",
      "before: 1000000021 / 14\n",
      " c: 14 / p: 13 / e: 39\n",
      "after: 1000000053 / 39\n",
      "\n",
      "before: 1000000022 / 15\n",
      " c: 15 / p: 14 / e: 40\n",
      "after: 1000000054 / 40\n",
      "\n",
      "before: 1000000023 / 16\n",
      " c: 16 / p: 15 / e: 41\n",
      "after: 1000000055 / 41\n",
      "\n",
      "before: 1000000024 / 17\n",
      " c: 17 / p: 16 / e: 42\n",
      "after: 1000000056 / 42\n",
      "\n",
      "before: 1000000025 / 18\n",
      " c: 18 / p: 17 / e: 43\n",
      "after: 1000000057 / 43\n",
      "\n",
      "before: 1000000026 / 19\n",
      " c: 19 / p: 18 / e: 44\n",
      "after: 1000000058 / 44\n",
      "\n",
      "before: 1000000027 / 20\n",
      " c: 20 / p: 19 / e: 45\n",
      "after: 1000000059 / 45\n",
      "\n",
      "before: 1000000028 / 21\n",
      " c: 21 / p: 20 / e: 46\n",
      "after: 1000000060 / 46\n",
      "\n",
      "before: 1000000029 / 22\n",
      " c: 22 / p: 21 / e: 47\n",
      "after: 1000000061 / 47\n",
      "\n",
      "before: 1000000030 / 23\n",
      " c: 23 / p: 22 / e: 48\n",
      "after: 1000000062 / 48\n",
      "\n",
      "before: 1000000031 / 24\n",
      " c: 24 / p: 23 / e: 49\n",
      "after: 1000000063 / 49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features2 = deepcopy(eval_features)\n",
    "\n",
    "previous_example_index = -1\n",
    "for fea in features2:\n",
    "    print(f\"before: {fea.unique_id} / {fea.example_index}\")\n",
    "    fea.unique_id = unique_id\n",
    "    unique_id += 1\n",
    "    \n",
    "    current_example_index = fea.example_index\n",
    "    print(f\" c: {current_example_index} / p: {previous_example_index} / e: {example_index}\")\n",
    "    if previous_example_index == current_example_index:\n",
    "        fea.example_index = previous_example_index\n",
    "    else:\n",
    "        previous_example_index = fea.example_index\n",
    "        fea.example_index = example_index\n",
    "        example_index += 1\n",
    "    print(f\"after: {fea.unique_id} / {fea.example_index}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous: 1000000000 / 0\n",
      "previous: 1000000001 / 1\n",
      "previous: 1000000002 / 1\n",
      "previous: 1000000003 / 1\n",
      "previous: 1000000004 / 1\n",
      "previous: 1000000005 / 1\n",
      "previous: 1000000006 / 2\n",
      "previous: 1000000007 / 3\n",
      "previous: 1000000008 / 4\n",
      "previous: 1000000009 / 5\n",
      "previous: 1000000010 / 5\n",
      "previous: 1000000011 / 6\n",
      "previous: 1000000012 / 6\n",
      "previous: 1000000013 / 7\n",
      "previous: 1000000014 / 7\n",
      "previous: 1000000015 / 8\n",
      "previous: 1000000016 / 8\n",
      "previous: 1000000017 / 9\n",
      "previous: 1000000018 / 9\n",
      "previous: 1000000019 / 10\n",
      "previous: 1000000020 / 10\n",
      "previous: 1000000021 / 11\n",
      "previous: 1000000022 / 11\n",
      "previous: 1000000023 / 12\n",
      "previous: 1000000024 / 13\n",
      "previous: 1000000025 / 14\n",
      "previous: 1000000026 / 15\n",
      "previous: 1000000027 / 16\n",
      "previous: 1000000028 / 17\n",
      "previous: 1000000029 / 18\n",
      "previous: 1000000030 / 19\n",
      "previous: 1000000031 / 20\n"
     ]
    }
   ],
   "source": [
    "for fea in features:\n",
    "    print(f\"previous: {fea.unique_id} / {fea.example_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for batch in eval_dataloader:\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "        example_indices = batch[3]\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        eval_feature = eval_features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        output = [tolist(o[i]) for o in outputs.values()]\n",
    "        start_logits, end_logits = output\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction_file = \"./predictions.json\"\n",
    "output_nbest_file = \"./nbest_predictions.json\"\n",
    "output_null_log_odds_file = \"./null_odds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compute_predictions_logits(\n",
    "    eval_examples,\n",
    "    eval_features,\n",
    "    all_results,\n",
    "    args.n_best_size,\n",
    "    args.max_answer_length,\n",
    "    args.do_lower_case,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    output_null_log_odds_file,\n",
    "    args.verbose_logging,\n",
    "    args.version_2_with_negative,\n",
    "    args.null_score_diff_threshold,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1_57059-1\n",
      "Answer: ÌïúÍµ≠Ï≤≠ÏÜåÎÖÑÎã®Ï≤¥ÌòëÏùòÌöåÏôÄ Ïó¨ÏÑ±Í∞ÄÏ°±Î∂Ä\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57060-1\n",
      "Answer: 22ÏùºÎ∂ÄÌÑ∞ 28Ïùº\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57061-1\n",
      "Answer: 'Ï≤≠ÏÜåÎÖÑÍ≥º Îâ¥ÎØ∏ÎîîÏñ¥'\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "c1_57062-1\n",
      "Answer: Í∏∞Ï°∞Í∞ïÏó∞ÏùÑ ÏãúÏûëÏúºÎ°ú Íµ≠Í∞ÄÎ≥Ñ Ï£ºÏ†úÍ¥ÄÎ†® ÏÇ¨Î°ÄÎ∞úÌëú, Í∑∏Î£π ÌÜ†Î°† Î∞è Ï†ÑÏ≤¥Ï¥ùÌöå, 'Ï≤≠ÏÜåÎÖÑÏÑ†Ïñ∏Î¨∏' ÏûëÏÑ± Î∞è Ï±ÑÌÉù Îì± Îã§ÏñëÌïú ÌîÑÎ°úÍ∑∏Îû®ÏùÑ Ïö¥ÏòÅÌïúÎã§.\n",
      "Predict: ÏÑ†Ïñ∏Î¨∏ÏùÑ Ï†ÑÎã¨Ìï† ÏòàÏ†ïÏù¥Îã§\n",
      "\n",
      "m5_306705-1\n",
      "Answer: ÏÉêÎ¶¨\n",
      "Predict: Ïú°ÏÉÅ Î≥ºÎßÅ ÏñëÍ∂Å Î¶¨Îì¨Ï≤¥Ï°∞ ÏóêÏñ¥Î°úÎπÖ ÏÑ†ÏàòÍ∂åÎåÄÌöå(Ïù¥Ìïò ÏïÑÏú°ÎåÄ)'ÏóêÏÑúÎäî\n",
      "\n",
      "c1_151305-1\n",
      "Answer: Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151306-1\n",
      "Answer: ÏßÄÎÇúÎã¨ 28Ïùº\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151307-1\n",
      "Answer: Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨Ïãú\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151308-1\n",
      "Answer: ‚ÄòÏ§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ‚ÄôÍ≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà(ÏïΩ 170ÎßåÏõê)ÏùÑ ÏàòÏó¨\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151309-1\n",
      "Answer: Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_151310-1\n",
      "Answer: Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§\n",
      "Predict: ÌöåÎ≥µÌï¥ ÏßÄÎÇú\n",
      "\n",
      "c1_151311-1\n",
      "Answer: ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤É\n",
      "Predict: ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É‚ÄùÏù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§\n",
      "\n",
      "c1_36509-1\n",
      "Answer: Ï°∞Îã¨Ï≤≠\n",
      "Predict: ÏÜåÏÜ° Í≤∞Í≥º Ïù∏Ïö©Î•†(Ï°∞Îã¨Ï≤≠ Ìå®ÏÜåÏú®)ÏùÄ ÌèâÍ∑† 86%Ïóê Îã¨ÌïúÎã§.Î∂ÄÏ†ïÎãπÏ†úÏ†ú\n",
      "\n",
      "c1_36510-1\n",
      "Answer: 2012ÎÖÑ\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36511-1\n",
      "Answer: ÏÜåÏÜ° Ï†úÏû¨ ÏûêÏ≤¥Ïóê Î¨∏Ï†úÍ∞Ä ÏûàÏóàÎã§Îäî ÏùòÎØ∏\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36512-1\n",
      "Answer: Í≥ÑÏïΩÏã¨ÏÇ¨ÌòëÏùòÌöåÏóêÏÑú ÎÇ¥Î∂Ä Ïã¨ÏùòÎ•º Í±∞Ï≥ê Î∂ÄÏ†ïÎãπÏóÖÏ≤¥Î°ú Îì±Î°ùÏùÑ ÌïòÍ≥† Ï†úÏû¨Î•º Ïã§Ïãú\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_36513-1\n",
      "Answer: ÏßÄÎ∞©Í≥ÑÏïΩÎ≤ïÏù¥ Í∞úÏ†ïÎêòÎ©¥ÏÑú ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê Ï†úÏû¨Í∂åÌïúÏù¥ Ï°∞Îã¨Ï≤≠ÏúºÎ°ú Ïù¥Í¥ÄÎêú Í≤É\n",
      "Predict: ÏßÄÎ∞©ÏûêÏπòÎã®Ï≤¥Ïùò Î∂ÄÏ†ïÎãπÏóÖÏûê\n",
      "\n",
      "c1_81296-1\n",
      "Answer: NASA\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81297-1\n",
      "Answer: 2004ÎÖÑ 1Ïõî\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81298-1\n",
      "Answer: ÌôîÏÑ±\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81299-1\n",
      "Answer: Ïö¥ÏÑùÏúºÎ°ú Î≥¥Ïù¥Îäî ÌäπÏù¥Ìïú ÏïîÏÑù\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81300-1\n",
      "Answer: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81301-1\n",
      "Answer: Ïò§ÌçºÌäúÎãàÌã∞Ïùò Ï†ÅÏô∏ÏÑ† Î∂ÑÍ¥ëÍ≥ÑÏù∏ Mini-TESÏùò Î∂ÑÏÑùÏóê Îî∞Î•¥Î©¥, Ïù¥ ÏïîÏÑùÏóêÏÑúÎäî ÌôîÏÑ± ÏïîÏÑùÏóêÏÑú ÎÇòÏò§Îäî Ï†ÑÌòïÏ†ÅÏù∏ Ïó¥ Ï†ÅÏô∏ÏÑ†Ïù¥ ÎÇòÏò§ÏßÄ ÏïäÏïòÍ∏∞ ÎïåÎ¨∏Ïóê\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81302-1\n",
      "Answer: Ïù¥ ÏïîÏÑù Í∑ºÏ≤òÏóê Î®∏Î¨ºÎ©¥ÏÑú Ïù¥Í≤ÉÏù¥ Ïö¥ÏÑùÏù∏ÏßÄÎ•º ÌôïÏã§ÌïòÍ≤å ÏïåÏïÑÎÇº Í≤É\n",
      "Predict: ÌôîÏÑ±ÏúºÎ°ú ÌïòÍ∞ïÌï† Îïå ÏÇ¨Ïö©Îêú Ïó¥ Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n",
      "c1_81303-1\n",
      "Answer: ÌëúÎ©¥Ïù¥ Ïö∏ÌâÅÎ∂àÌâÅÌïú ÌôàÏù¥ ÌååÏó¨ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê\n",
      "Predict: Ï∞®Ìèê Î∞©Ìå®(heat shield)Ïóê Ï†ëÍ∑ºÌïòÎ©¥ÏÑú Ïù¥ ÏïîÏÑùÏùÑ Ï≤òÏùå Î∞úÍ≤¨ÌñàÎã§.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(eval_examples, predictions.items()):\n",
    "    if i.qas_id == j[0]:\n",
    "        print(i.qas_id)\n",
    "        print(f\"Answer: {i.answers[0]['text']}\")\n",
    "        print(f\"Predict: {j[1]}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(i.qas_id, j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = squad_evaluate(eval_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 0.0),\n",
       "             ('f1', 5.556363636363637),\n",
       "             ('total', 25),\n",
       "             ('HasAns_exact', 0.0),\n",
       "             ('HasAns_f1', 5.556363636363637),\n",
       "             ('HasAns_total', 25),\n",
       "             ('best_exact', 0.0),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 5.556363636363637),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why there is 0 to predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_rev = {v: k for k, v in tokenizer.vocab.items()}\n",
    "tostring = lambda x: \" \".join(x).replace(\" ##\", \"\").replace(\"[PAD]\", \"\").strip()\n",
    "def show_original(inputs):\n",
    "    tokens = [vocab_rev[i.item()] for i in inputs[\"input_ids\"]]\n",
    "    s, e = inputs[\"start_positions\"].item(), inputs[\"end_positions\"].item()\n",
    "    print(s, e)\n",
    "    print(\"Answer: \", tostring(tokens[s:(e+1)]))\n",
    "    print(tostring(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "103 112\n",
      "Answer:  Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë\n",
      "[CLS] Ï§ëÍµ≠ÏóêÏÑú ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†ÏßÑ ÏÇ¨ÎûåÏùÄ ÎàÑÍµ¨Ïïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅ [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï§ëÍµ≠ÏóêÏÑú ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†ÏßÑ ÏÇ¨ÎûåÏùÄ ÎàÑÍµ¨Ïïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "114 116\n",
      "Answer:  ÏßÄÎÇúÎã¨ 28Ïùº\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Ï∂îÎùΩÌïòÎäî ÏïÑÏù¥Î•º Íµ¨ÌïòÍ≥† ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑÍ±¥ Ïñ∏Ï†úÏïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Ï∂îÎùΩÌïòÎäî ÏïÑÏù¥Î•º Íµ¨ÌïòÍ≥† ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑÍ±¥ Ïñ∏Ï†úÏïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "87 91\n",
      "Answer:  Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨Ïãú\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Í≥≥ÏùÄ Ïñ¥ÎîîÏïº ? [SEP] Ï§ëÍµ≠Ïùò Ìïú Ïó¨ÏÑ± Í≤ΩÏ∞∞Ïù¥ ÏïÑÌååÌä∏ÏóêÏÑú Ï∂îÎùΩÌïòÎçò 3ÏÑ∏ ÏïÑÏù¥Î•º ÏÇ¥Î¶¨Í≥† ÏûêÏã†ÏùÄ ÌòºÏàòÏÉÅÌÉúÏóê Îπ†Ï°åÏäµÎãàÎã§ . ÏùòÏù∏ ( Áæ© ‰∫∫ ) Ïùò ÏÜåÏãùÏù¥ ÏïåÎ†§ÏßÄÏûê Í∞ÅÎ∞ïÌïú Ï§ëÍµ≠ ÏÇ¨ÌöåÏóê ÌÅ∞ Î∞òÌñ•ÏùÑ ÏùºÏúºÌÇ§Í≥† ÏûàÏäµÎãàÎã§ . 5Ïùº Í∑ÄÏ£ºÎèÑÏãúÎßù Îì± Ï§ëÍµ≠ ÌòÑÏßÄ Ïñ∏Î°†Ïóê Îî∞Î•¥Î©¥ Íµ¨Ïù¥Ï†ÄÏö∞ÏÑ± Ïπ¥ÏùºÎ¶¨ÏãúÏóê Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Ï≤úÏ§ëÌïë ( 49 ) ÏùÄ ÏßÄÎÇúÎã¨ 28Ïùº Ìïú ÏïÑÌååÌä∏ÏóêÏÑú ÎπÑÏÉÅ ÏÉÅÌô©Ïù¥ Î∞úÏÉùÌñàÎã§Îäî Ïó∞ÎùΩÏùÑ Î∞õÍ≥† ÌòÑÏû•ÏúºÎ°ú Ìñ•ÌñàÏäµÎãàÎã§ . ÎèÑÏ∞©ÌñàÏùÑ Îïå ÏïÑÌååÌä∏ 4Ï∏µ Ï∞ΩÎ¨∏ÏóêÏÑú Ïó¨Ïûê ÏïÑÏù¥Í∞Ä Îß§Îã¨Î†§ ÏûàÏóàÏäµÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ [SEP]\n",
      "\n",
      "0 0\n",
      "Answer:  [CLS]\n",
      "[CLS] Ï≤úÏ§ëÌïëÏî®Í∞Ä Î≥¥Ï°∞ ÍµêÌÜµ Í≤ΩÏ∞∞Î°ú ÏùºÌïòÎäî Í≥≥ÏùÄ Ïñ¥ÎîîÏïº ? [SEP]ÎãàÎã§ . Í≥ßÏù¥Ïñ¥ ÏïÑÏù¥Îäî ÏÜêÏóê ÌûòÏù¥ Îπ†ÏßÄÎ©¥ÏÑú Î∞ëÏúºÎ°ú Ï∂îÎùΩÌñàÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÍ≥º Îã§Î•∏ ÏÑ∏Î™ÖÏùò Ïù¥ÏõÉÎì§Ïù¥ Îã¨Î†§Í∞îÏäµÎãàÎã§ . Í∑∏Î¶¨Í≥† ÏïÑÏù¥Îäî Î∞îÎã•Ïù¥ ÏïÑÎãàÎùº Ï≤úÏ§ëÌïëÏùò ÌåîÏóê Îñ®Ïñ¥Ï°åÏäµÎãàÎã§ . Ï§ëÍ∞Ñ ÎπÑÎßâÏù¥ Ï≤úÎßâ ÎïåÎ¨∏Ïóê ÏÜçÎèÑÍ∞Ä Ï§ÑÍ∏∞Îäî ÌñàÏßÄÎßå Ï∂îÎùΩÏùò Ï∂©Í≤©ÏùÄ Ï≤úÏ§ëÌïëÏù¥ Í≥†Ïä§ÎûÄÌûà Í∞êÎãπÌï¥Ïïº ÌñàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ï¶âÏãú Î≥ëÏõêÏúºÎ°ú ÏòÆÍ≤®Ï†∏ ÏπòÎ£åÎ•º Î∞õÏïòÏäµÎãàÎã§ . Îã§Î¶¨ Í≥®Ï†àÎ°ú Í∑∏Î¶¨ Ïã¨Í∞ÅÌïú ÏÉÅÌô©ÏùÄ ÏïÑÎãàÎùºÍ≥† Ìï©ÎãàÎã§ . ÌïòÏßÄÎßå ÏÉùÎ™ÖÏùò ÏùÄÏù∏Ïù¥Ïûê ÏòÅÏõÖÏùÄ Ïª§Îã§ÎûÄ ÎåìÍ∞ÄÎ•º ÏπòÎü¨Ïïº ÌñàÎã§ . ÎáåÏ∂úÌòàÎ°ú Ïù∏Ìïú ÏùòÏãùÎ∂àÎ™Ö ÏÉÅÌÉúÏóê Îπ†ÏßÑ Í≤ÉÏù¥Îã§ . Îã§ÌñâÌûà Ïù¥ÌãÄ Í∞ÑÏùò ÏΩîÎßà ÏÉÅÌÉú Ïù¥ÌõÑ ÏùòÏãùÏùÑ ÌöåÎ≥µÌï¥ ÏßÄÎÇú 2ÏùºÎ∂ÄÌÑ∞ Ï§ëÌôòÏûêÏã§ÏóêÏÑú ÏπòÎ£åÎ•º Î∞õÍ≥† ÏûàÏäµÎãàÎã§ . ÏïÑÏù¥Îäî Ïó¥Ïá†Í≥µÏù¥ Î¨∏ÏùÑ Îî∞Îäî ÏÜåÎ¶¨Ïóê Í≤ÅÏùÑ Î®πÍ≥† Ï∞ΩÎ¨∏ Î∞ñÏúºÎ°ú ÎèÑÎßùÏùÑ ÏπòÎ†§Îã§ ÏÇ¨Í≥†Î•º ÎãπÌïú Í≤ÉÏúºÎ°ú Ï†ÑÌï¥Ï°åÏäµÎãàÎã§ . ÏïÑÏù¥Í∞Ä Ïû†Îì† ÏÇ¨Ïù¥ ÎèåÎ≥¥Îçò ÏïÑÏù¥Ïùò Ìï†Î®∏ÎãàÍ∞Ä Ïì∞Î†àÍ∏∞Î•º Î≤ÑÎ¶¨Îü¨ ÎÇòÍ∞îÎã§Í∞Ä Î¨∏Ïù¥ Ïû†Í∏∞Îäî Î∞îÎûåÏóê Ïó¥Ïá†Í≥µÏùÑ Î∂àÎ†ÄÎçò Í≤ÉÏûÖÎãàÎã§ . ÏïÑÏù¥Ïùò ÏóÑÎßàÎäî ‚Äú Ï≤úÏ§ëÌïëÏùò ÎèÑÏõÄÏù¥ ÏóÜÏóàÎã§Î©¥ ÏïÑÏù¥Îäî Ï£ΩÏóàÏùÑ Í≤É ‚Äù Ïù¥ÎùºÎ©∞ Îî∏ÏùÑ Íµ¨Ìï¥Ï§Ä Ï≤úÏ§ëÌïëÏóêÍ≤å Í∞êÏÇ¨Ïùò ÎúªÏùÑ Ï†ÑÌñàÏäµÎã§ . Ïπ¥ÏùºÎ¶¨Ïãú Ï†ïÎ∂Ä ÎåÄÌëúÏôÄ Í≥µÏïàÎ∂Ä Í¥ÄÍ≥ÑÏûêÎì§ÎèÑ Ï≤úÏ§ëÌïëÏù¥ ÏûÖÏõêÌïú Î≥ëÏõêÏùÑ Ï∞æÏïÑ ÏúÑÎ°úÌïòÍ≥† ÌöåÎ≥µÎê†ÎïåÍπåÏßÄ ÎèÑÏõÄÏùÑ ÏïÑÎÅºÏßÄ ÏïäÍ≤†Îã§Í≥† Î∞ùÌòîÏäµÎãàÎã§ . Ï≤úÏ§ëÌïëÏùò ÏÑ†Ìñâ ÏÇ¨Ïã§ÏùÑ Ï†ëÌïú Ï§ëÍµ≠ Í∏∞ÏóÖ ÏïåÎ¶¨Î∞îÎ∞îÎèÑ ‚Äò Ï§ëÍµ≠Ïùò Ï¢ãÏùÄ Ïù¥ÏõÉÏÉÅ ‚Äô Í≥º Ìï®Íªò ÏÉÅÍ∏à 1Îßå ÏúÑÏïà ( ÏïΩ 170ÎßåÏõê ) ÏùÑ ÏàòÏó¨ÌïòÍ∏∞Î°ú ÌñàÏäµÎãàÎã§ . [ ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅ ] ÏùÄ Ï†êÏ†ê Í∞ÅÎ∞ïÌï¥ÏßÄÎäî ÏÑ∏ÏÉÅÏóê Ìù¨ÎßùÍ≥º ÎØøÏùåÏùÑ Ï£ºÎäî Ïù¥Îì§Ïùò Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§ . ÌûòÎì§Í≥† ÏßÄÏπ† Îïå ÏïÑÏßÅ ÏÇ¥ÎßåÌïú ÏÑ∏ÏÉÅÏùÑ ÎßåÎì§Ïñ¥Í∞ÄÎäî ‚Äò ÏïÑÏÇ¥ÏÑ∏ ‚Äô ÏÇ¨ÎûåÎì§Ïùò Î™©ÏÜåÎ¶¨Î•º Îì§Ïñ¥Î≥¥ÏÑ∏Ïöî . Îî∞ÎúªÌïú ÏÑ∏ÏÉÅÏùÑ ÍøàÍæ∏Îäî ÎèÖÏûê Ïó¨Îü¨Î∂ÑÏùò Ï†úÎ≥¥Î•º Í∏∞Îã§Î¶ΩÎãàÎã§ . ÎßπÍ≤ΩÌôò Í∏∞Ïûê khmaeng @ kmib . co . kr [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxes = [5, 6, 7, 8, 9, 10]\n",
    "for i, idx in enumerate(idxes):\n",
    "    batch = train_dataset[idx]\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1],\n",
    "        \"token_type_ids\": batch[2],\n",
    "        \"start_positions\": batch[3],\n",
    "        \"end_positions\": batch[4],\n",
    "    }\n",
    "    if i % 2 == 0:\n",
    "        print(\"----------------------------\"*3)\n",
    "    show_original(inputs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If over 512 tokens, it will generate sentences from the back like: `tokens[-512:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    ElectraForQuestionAnswering, \n",
    "    ElectraConfig, \n",
    "    ElectraTokenizer,\n",
    "    AdamW,\n",
    "    squad_convert_examples_to_features,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate\n",
    ")\n",
    "# typing\n",
    "from transformers.data.processors import SquadFeatures\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"test_train*.json\"\n",
    "val_file = \"test_val*.json\"\n",
    "repo_path = Path().absolute().parent\n",
    "data_path = repo_path.parent / \"data\" / \"AIhub\" / \"QA\"\n",
    "ckpt_path = repo_path.parent / \"ckpt\"\n",
    "\n",
    "args_dict = {\n",
    "    \"task\": \"AIHub_QA\",\n",
    "    \"data_path\": data_path,\n",
    "    \"ckpt_path\": ckpt_path,\n",
    "    \"train_file\": train_file,\n",
    "    \"val_file\": val_file,\n",
    "    \"cache_file\": \"test_{}_cache_{}\",\n",
    "    \"random_seed\": 77,\n",
    "    \"threads\": 4,\n",
    "    \"version_2_with_negative\": False,\n",
    "    \"null_score_diff_threshold\": 0.0,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"doc_stride\": 128,\n",
    "    \"max_query_length\": 64,\n",
    "    \"max_answer_length\": 30,\n",
    "    \"n_best_size\": 20,\n",
    "    \"verbose_logging\": True,\n",
    "    \"do_lower_case\": False,\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"warmup_proportion\": 0,\n",
    "    \"model_type\": \"koelectra-base-v3\",\n",
    "    \"model_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
    "    \"output_dir\": \"koelectra-base-v3-korquad-ckpt\",\n",
    "    \"seed\": 42,\n",
    "    \"train_batch_size\": 2,\n",
    "    \"eval_batch_size\": 3,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"output_prediction_file\": \"predictions/predictions_{}.json\",\n",
    "    \"output_nbest_file\": \"nbest_predictions/nbest_predictions_{}.json\",\n",
    "    \"output_null_log_odds_file\": \"null_odds/null_odds_{}.json\",\n",
    "}\n",
    "\n",
    "for arg in [\"output_prediction_file\", \"output_nbest_file\", \"output_null_log_odds_file\"]:\n",
    "    p = args_dict[\"ckpt_path\"] / args_dict[arg]\n",
    "    if not p.parent.exists():\n",
    "        p.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten(li):\n",
    "    for ele in li:\n",
    "        if isinstance(ele, list):\n",
    "            yield from flatten(ele)\n",
    "        else:\n",
    "            yield ele\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.config = ElectraConfig.from_pretrained(self.hparams.model_name_or_path)\n",
    "        self.model = ElectraForQuestionAnswering.from_pretrained(\n",
    "            self.hparams.model_name_or_path, \n",
    "            config=self.config\n",
    "        )\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(self.hparams.model_name_or_path)\n",
    "        # create dataset and cache it\n",
    "        self.train_files = []\n",
    "        self.val_files = []\n",
    "        self.create_dataset_all(state=\"train\")\n",
    "        self.create_dataset_all(state=\"val\")\n",
    "\n",
    "        # \n",
    "        self.all_examples, self.all_features = [], []\n",
    "\n",
    "        # function\n",
    "        self.tolist = lambda x: x.detach().cpu().tolist()\n",
    "\n",
    "    def create_dataset_all(self, state:str):\n",
    "        self.example_index = 0\n",
    "        self.unique_id = 1000000000\n",
    "        if state == \"train\":\n",
    "            file_str = self.hparams.train_file\n",
    "        elif state == \"val\":\n",
    "            file_str = self.hparams.val_file\n",
    "        else:\n",
    "            raise ValueError(\"state should be train or val\")\n",
    "        \n",
    "        file_iter = sorted(self.hparams.data_path.glob(file_str), key=lambda x: int(x.name.strip(\".json\").split(\"_\")[-1]))\n",
    "        for path in file_iter:\n",
    "            filename = path.name\n",
    "            idx = int(filename.strip(\".json\").split(\"_\")[-1])\n",
    "            self.create_dataset(path.name, idx, state)\n",
    "\n",
    "    def create_dataset(self, filename:str, idx:int, state:str):\n",
    "        cache_file = self.hparams.cache_file.format(state, idx)\n",
    "        print(f\"[INFO] Processing: {filename} | Cache file name: {cache_file}\")\n",
    "        processed_file = self.hparams.ckpt_path / cache_file\n",
    "        if processed_file.exists():\n",
    "            print(f\"[INFO] cache file already exists! passing the procedure\")\n",
    "            print(f\"[INFO] Path: {processed_file}\")\n",
    "            if state == \"train\":\n",
    "                self.train_files.append(cache_file)\n",
    "            elif state == \"val\":\n",
    "                self.val_files.append(cache_file)\n",
    "            else:\n",
    "                raise ValueError(\"state should be train or val\")\n",
    "            return None\n",
    "        else:\n",
    "            processor = SquadV2Processor()\n",
    "            if state == \"train\":\n",
    "                process_fn = processor.get_train_examples\n",
    "                is_training = True\n",
    "                self.train_files.append(cache_file)\n",
    "            elif state == \"val\":\n",
    "                process_fn = processor.get_dev_examples\n",
    "                is_training = False\n",
    "                self.val_files.append(cache_file)\n",
    "            else:\n",
    "                raise ValueError(\"state should be train or val\")\n",
    "\n",
    "            examples = process_fn(\n",
    "                data_dir=self.hparams.data_path, \n",
    "                filename=filename\n",
    "            )\n",
    "\n",
    "            features = squad_convert_examples_to_features(\n",
    "                examples=examples,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_seq_length=self.hparams.max_seq_length,\n",
    "                doc_stride=self.hparams.doc_stride,\n",
    "                max_query_length=self.hparams.max_query_length,\n",
    "                is_training=is_training,\n",
    "                return_dataset=False,\n",
    "                threads=self.hparams.threads,\n",
    "            )\n",
    "            # need to fix all `example_index` and `unique_id` since splitted the dataset only on validation dataset\n",
    "            self.fix_unique_id(features, state)\n",
    "            dataset = self.convert_to_tensor(state, features)\n",
    "            cache = dict(dataset=dataset, examples=examples, features=features)\n",
    "            torch.save(cache, processed_file)\n",
    "            print(f\"[INFO] cache file saved! {processed_file}\")\n",
    "\n",
    "    def convert_to_tensor(self, state:str, features:List[SquadFeatures]):\n",
    "        \"\"\"\n",
    "        Reference: https://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/squad.py\n",
    "        Arguments:\n",
    "            state {str} -- [description]\n",
    "        \"\"\"        \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "        \n",
    "        if state == \"train\":\n",
    "            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "            dataset = TensorDataset(\n",
    "                all_input_ids, all_attention_masks, all_token_type_ids, all_start_positions, all_end_positions\n",
    "            )\n",
    "        elif state == \"val\":\n",
    "            all_unique_ids = torch.tensor([f.unique_id for f in features], dtype=torch.long)\n",
    "            dataset = TensorDataset(\n",
    "                all_input_ids, all_attention_masks, all_token_type_ids, all_unique_ids\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"state should be train or val\")\n",
    "        return dataset\n",
    "\n",
    "    def fix_unique_id(self, features:list, state:str=\"val\"):\n",
    "        if state == \"val\":\n",
    "            previous_example_index = -1 \n",
    "            for fea in tqdm(features, total=len(features), desc=\"fixing index and ids\"):\n",
    "                fea.unique_id = self.unique_id\n",
    "                self.unique_id += 1\n",
    "                \n",
    "                current_example_index = fea.example_index\n",
    "                if previous_example_index == current_example_index:\n",
    "                    fea.example_index = previous_example_index\n",
    "                else:\n",
    "                    previous_example_index = fea.example_index\n",
    "                    fea.example_index = self.example_index\n",
    "                    self.example_index += 1\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def load_cache(self, filename:str, return_dataset:bool=True):\n",
    "        processed_file = self.hparams.ckpt_path / filename\n",
    "        cache = torch.load(processed_file)\n",
    "        dataset, examples, features = cache[\"dataset\"], cache[\"examples\"], cache[\"features\"]\n",
    "\n",
    "        if return_dataset:\n",
    "            return dataset\n",
    "        else:\n",
    "            return examples, features\n",
    "\n",
    "    def create_dataloader(self, state:str=\"train\"):\n",
    "        if state == \"train\":\n",
    "            shuffle = True\n",
    "            batch_size = self.hparams.train_batch_size\n",
    "            file_list = self.train_files\n",
    "        elif state == \"val\":\n",
    "            shuffle = False\n",
    "            batch_size = self.hparams.eval_batch_size\n",
    "            file_list = self.val_files\n",
    "        else:\n",
    "            raise ValueError(\"state should be train or val\")\n",
    "\n",
    "        file_loader = (self.load_cache(filename=file, return_dataset=True) for file in file_list)\n",
    "        loaders = []\n",
    "        self.val_dataset_length = []\n",
    "        for dataset in file_loader: \n",
    "            dataloader = DataLoader(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=self.hparams.threads\n",
    "            )\n",
    "            loaders.append(dataloader)\n",
    "            self.val_dataset_length.append(len(dataset))\n",
    "        return loaders\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(state=\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(state=\"val\")\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = list(map(torch.cat, zip(*batch)))\n",
    "        inputs_ids, attention_mask, token_type_ids, start_positions, end_positions = batch\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=inputs_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        return  {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx):\n",
    "        # batch = single dataloader batch not multiple dataloader\n",
    "        inputs_ids, attention_mask, token_type_ids, data_unique_ids = batch\n",
    "        outputs = self(\n",
    "            input_ids=inputs_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=None,\n",
    "            end_positions=None\n",
    "        )\n",
    "\n",
    "        # outputs.values: [(B, H), (B, H)] > batch_results: (B, 2, H)\n",
    "        # B = len(datasets) * batch_size\n",
    "        batch_results = []\n",
    "        for i, unique_id in enumerate(data_unique_ids.detach().cpu().tolist()):\n",
    "            output = [self.tolist(o[i]) for o in outputs.values()]\n",
    "            start_logits, end_logits = output\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "            batch_results.append(result)\n",
    "\n",
    "        # for i, example_index in enumerate(example_indices):\n",
    "        #     eval_feature = self.eval_features[example_index.item()]\n",
    "        #     unique_id = int(eval_feature.unique_id)\n",
    "        #     output = [self.tolist(o[i]) for o in outputs.values()]\n",
    "        #     start_logits, end_logits = output\n",
    "        #     result = SquadResult(unique_id, start_logits, end_logits)\n",
    "        #     batch_results.append(result)\n",
    "            \n",
    "        return batch_results\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if (self.all_examples == []) or (self.all_features == []):\n",
    "            for file in self.val_files:\n",
    "                examples, features = self.load_cache(filename=file, return_dataset=False)\n",
    "                self.all_examples.extend(examples)\n",
    "                self.all_features.extend(features)\n",
    "                del examples\n",
    "                del features\n",
    "\n",
    "        all_results = list(flatten(outputs))\n",
    "        # TODO: See if needed?\n",
    "        # outputs: [(B, 2, H)] : start_logits, end_logits list\n",
    "        # B = len(datasets) * batch_size\n",
    "        # all_results = []\n",
    "        # for res in outputs:  # res: (B, 2, H)\n",
    "        #     start_logits, end_logits = res\n",
    "        #     idx = torch.arange(self.hparams.train_batch_size).repeat(len(self.val_files), 1)  # len(dataset), batch_size\n",
    "        #     example_idx_to_add = torch.LongTensor([0] + self.val_dataset_length[:-1]).unsqueeze(1)\n",
    "        #     idx = (idx + example_idx_to_add).view(-1)  # B\n",
    "        #     for k in idx:\n",
    "        #         unique_id = self.all_features[k].unique_id\n",
    "        #         result = SquadResult(unique_id, start_logits, end_logits)\n",
    "        #     all_results.append(result)\n",
    "        \n",
    "        # https://huggingface.co/transformers/_modules/transformers/data/processors/squad.html\n",
    "        # TODO: Cannot find the key unique_id\n",
    "        # BUG: must set argument of `trainer: num_sanity_val_steps=0` to avoid error.\n",
    "\n",
    "        predictions = compute_predictions_logits(\n",
    "            self.all_examples,\n",
    "            self.all_features,\n",
    "            all_results,\n",
    "            self.hparams.n_best_size,\n",
    "            self.hparams.max_answer_length,\n",
    "            self.hparams.do_lower_case,\n",
    "            self.hparams.ckpt_path / self.hparams.output_prediction_file.format(self.global_step),\n",
    "            self.hparams.ckpt_path / self.hparams.output_nbest_file.format(self.global_step),\n",
    "            self.hparams.ckpt_path / self.hparams.output_null_log_odds_file.format(self.global_step),\n",
    "            self.hparams.verbose_logging,\n",
    "            self.hparams.version_2_with_negative,\n",
    "            self.hparams.null_score_diff_threshold,\n",
    "            self.tokenizer,\n",
    "        )\n",
    "        results = squad_evaluate(self.all_examples, predictions)\n",
    "        accuracy = results[\"exact\"]\n",
    "        f1 = results[\"f1\"]\n",
    "        self.log(\"accuracy\", accuracy, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"f1\", f1, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        t_total = self.total_steps()\n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "                \"weight_decay\": 0.0\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            params=optimizer_grouped_parameters, \n",
    "            lr=self.hparams.learning_rate, \n",
    "            eps=self.hparams.adam_epsilon\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer, \n",
    "            num_warmup_steps=int(t_total * self.hparams.warmup_proportion), \n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def total_steps(self):\n",
    "        r\"\"\"\n",
    "        source: https://github.com/PyTorchLightning/pytorch-lightning/issues/1038\n",
    "        \"\"\"\n",
    "        return len(self.train_dataloader()) * self.hparams.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args_dict):\n",
    "    print(\"[INFO] Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"[INFO] Seed:\", args_dict[\"random_seed\"])\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        filename=\"epoch{epoch}-f1{f1:.4f}\",\n",
    "        monitor=\"f1\",\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "    pl.seed_everything(args_dict[\"random_seed\"])\n",
    "    model = Model(**args_dict)\n",
    "    \n",
    "    print(\"[INFO] Start FineTuning\")\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[checkpoint_callback],\n",
    "        max_epochs=args_dict[\"num_train_epochs\"],\n",
    "        deterministic=torch.cuda.is_available(),\n",
    "        gpus=-1 if torch.cuda.is_available() else None,\n",
    "        num_sanity_val_steps=0\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using PyTorch Ver 1.6.0\n",
      "[INFO] Seed: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | ElectraForQuestionAnswering | 112 M \n",
      "------------------------------------------------------\n",
      "112 M     Trainable params\n",
      "0         Non-trainable params\n",
      "112 M     Total params\n",
      "449.329   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: test_train_0.json | Cache file name: test_train_cache_0\n",
      "[INFO] cache file already exists! passing the procedure\n",
      "[INFO] Path: /home/simonjisu/code/ckpt/test_train_cache_0\n",
      "[INFO] Processing: test_train_1.json | Cache file name: test_train_cache_1\n",
      "[INFO] cache file already exists! passing the procedure\n",
      "[INFO] Path: /home/simonjisu/code/ckpt/test_train_cache_1\n",
      "[INFO] Processing: test_val_0.json | Cache file name: test_val_cache_0\n",
      "[INFO] cache file already exists! passing the procedure\n",
      "[INFO] Path: /home/simonjisu/code/ckpt/test_val_cache_0\n",
      "[INFO] Processing: test_val_1.json | Cache file name: test_val_cache_1\n",
      "[INFO] cache file already exists! passing the procedure\n",
      "[INFO] Path: /home/simonjisu/code/ckpt/test_val_cache_1\n",
      "[INFO] Start FineTuning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2967ba1984254c26b83549e16b516c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): ElectraForQuestionAnswering(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
